{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mtjon/infompr-group-project/blob/all_in_one/all_in_one.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this notebook elemets were taken from:\n",
        "1. https://www.tensorflow.org/text/tutorials/nmt_with_attention, and;\n",
        "2. https://www.tensorflow.org/text/tutorials/transformer\n",
        "\n",
        "For the RNN + GRU, bi-directional layer was removed. For the transformer, all sub-classing has been copied, as well as custom learning schedule.\n",
        "\n",
        "Data-preprocessing has been adopted to fit the dataset used, which can be found here:\n",
        "- Data loading: https://github.com/EagleW/Writing-editing-Network/blob/master/split_data.py\n",
        "- Dataset: https://github.com/EagleW/ACL_titles_abstracts_dataset\n",
        "\n",
        "We've manually adjusted the dataset to remove the empty rows between title-abstract pairs, and adjusted the data loading code to work with the changes accordingly."
      ],
      "metadata": {
        "id": "Q7k2Rb8uLP14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing and loading Required packages"
      ],
      "metadata": {
        "id": "YIdpCzRxMTLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the most re version of TensorFlow to use the improved\n",
        "# masking support for `tf.keras.layers.MultiHeadAttention`.\n",
        "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n",
        "!pip uninstall -y -q tensorflow keras tensorflow-estimator tensorflow-text\n",
        "!pip install -q tensorflow_datasets\n",
        "# !pip install -q -U tensorflow-text==2.9.* tensorflow==2.9.*\n",
        "!pip install -q -U tensorflow-text==2.9.* tensorflow==2.9.*\n",
        "!pip install einops"
      ],
      "metadata": {
        "id": "S-xUpwPLMTBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U tensorflow-text tensorflow"
      ],
      "metadata": {
        "id": "B5nW1nEDyIOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list | grep tensorflow"
      ],
      "metadata": {
        "id": "Yzx5IyuayQkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_text\n",
        "import tensorflow_text as tf_text\n",
        "\n",
        "# For data loading and shuffling\n",
        "import random\n",
        "from random import shuffle\n",
        "\n",
        "# for shape checking -> used in RNN and GRU, see class below\n",
        "import einops"
      ],
      "metadata": {
        "id": "Uy5TSM4NMeVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "class ShapeChecker():\n",
        "  def __init__(self):\n",
        "    # Keep a cache of every axis-name seen\n",
        "    self.shapes = {}\n",
        "\n",
        "  def __call__(self, tensor, names, broadcast=False):\n",
        "    if not tf.executing_eagerly():\n",
        "      return\n",
        "\n",
        "    parsed = einops.parse_shape(tensor, names)\n",
        "\n",
        "    for name, new_dim in parsed.items():\n",
        "      old_dim = self.shapes.get(name, None)\n",
        "      \n",
        "      if (broadcast and new_dim == 1):\n",
        "        continue\n",
        "\n",
        "      if old_dim is None:\n",
        "        # If the axis name is new, add its length to the cache.\n",
        "        self.shapes[name] = new_dim\n",
        "        continue\n",
        "\n",
        "      if new_dim != old_dim:\n",
        "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
        "                         f\"    found: {new_dim}\\n\"\n",
        "                         f\"    expected: {old_dim}\\n\")"
      ],
      "metadata": {
        "id": "bR5vmAK5Msbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "metadata": {
        "id": "acobDY2eMGRx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "TMkQ7LwANOOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #Necessary when upload fails (Firefox+Colab issue)\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "metadata": {
        "id": "seytZR35cw1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# modified data externally to remove all blank lines\n",
        "def write_data(f, ext, mod):\n",
        "    # Code van https://github.com/EagleW/Writing-editing-Network/blob/master/split_data.py\n",
        "    file1=open(f, 'r')\n",
        "    lines=file1.readlines()\n",
        "    print(len(lines))\n",
        "    file1.close()\n",
        "    abs_t = []\n",
        "    abstracts = []\n",
        "    titles = []\n",
        "    i = 0\n",
        "    # TODO: possibly generates wrong tibs/abs when encounter\n",
        "    for line in lines:\n",
        "        if i % mod == 0:\n",
        "            titles.append(line)\n",
        "        elif i % mod == 1:\n",
        "            abstracts.append(line)\n",
        "        i += 1\n",
        "    for i in range(len(abstracts)):\n",
        "        if len(titles[i]) > 0 and len(abstracts[i]) > 0:\n",
        "            h_a_pair = (titles[i], abstracts[i])\n",
        "            abs_t.append(h_a_pair)\n",
        "    shuffle(abs_t)\n",
        "    total = len(abs_t)\n",
        "    dev = total//10\n",
        "    train  = total - dev - dev\n",
        "    i = 0\n",
        "    file1=open(\"val{}.txt\".format(ext), 'w')\n",
        "    for i in range(dev):\n",
        "        file1.writelines(abs_t[i][0])\n",
        "        file1.writelines(abs_t[i][1])\n",
        "    #    file1.writelines(\"\\n\")\n",
        "    file1.close()\n",
        "    file1=open(\"test{}.txt\".format(ext), 'w')\n",
        "    for i in range(dev, 2 * dev):\n",
        "        file1.writelines(abs_t[i][0])\n",
        "        file1.writelines(abs_t[i][1])\n",
        "    #    file1.writelines(\"\\n\")\n",
        "    file1.close()\n",
        "    file1=open(\"train{}.txt\".format(ext), 'w')\n",
        "    for i in range(2 * dev, total):\n",
        "        file1.writelines(abs_t[i][0])\n",
        "        file1.writelines(abs_t[i][1])\n",
        "    #    file1.writelines(\"\\n\")\n",
        "    file1.close()\n",
        "\n",
        "def get_abs_and_titles_from_raw(path_to_file):\n",
        "    abstracts, titles = [], []\n",
        "    abscount, titcount = 0, 0\n",
        "    with open(path_to_file) as data:\n",
        "        lines = data.readlines()\n",
        "        print(len(lines))\n",
        "        for abs in lines[1::2]:\n",
        "            abstracts.append(abs.strip())\n",
        "            abscount += 1\n",
        "        for title in lines[0::2]:\n",
        "            # // TODO: check if we need to add start and end tokens\n",
        "            titles.append(title.strip())\n",
        "            titcount+=1\n",
        "            # titles.append('[START] ' + title.strip() + ' [END]')\n",
        "    print(abscount, titcount)\n",
        "    # print(titles[-1],'\\n',titles[-2])\n",
        "    return abstracts, titles"
      ],
      "metadata": {
        "id": "7jhRoUQrMHwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading data\n",
        "\n",
        "Make sure to have the modified dataset uploaden (i.e., ACL title+abs data set with empty rows removed).\n",
        "\n",
        "### Writing and loading text files"
      ],
      "metadata": {
        "id": "ph95j8o-NRHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "write_data(\"acl_titles_and_abstracts_mod.txt\", \"\", 2) # writes train, val, test.txt"
      ],
      "metadata": {
        "id": "WYrrf-LoMH40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text, train_labels = get_abs_and_titles_from_raw('train.txt')\n",
        "val_text, val_labels = get_abs_and_titles_from_raw('val.txt')\n",
        "test_text, test_labels = get_abs_and_titles_from_raw('test.txt')"
      ],
      "metadata": {
        "id": "yKWdenZIMIAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sanity checks"
      ],
      "metadata": {
        "id": "nhYaxPDwMIJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_labels[0])\n",
        "print(train_text[0])"
      ],
      "metadata": {
        "id": "Hvc13xoNMIhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(val_labels[0])\n",
        "print(val_text[0])"
      ],
      "metadata": {
        "id": "jU9dcSVdMIpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_labels[0])\n",
        "print(test_text[0])"
      ],
      "metadata": {
        "id": "87huQf8NMIxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batching"
      ],
      "metadata": {
        "id": "GEYzfYsdMI4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_text, train_labels)).batch(BATCH_SIZE)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_text, val_labels)).batch(BATCH_SIZE)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_text, test_labels)).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "Xz5sFZzTMJI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counter = 0\n",
        "for abs, tit in train_dataset.take(1):\n",
        "    print('> Abstract examples:')\n",
        "    for i, a in enumerate(abs.numpy()):\n",
        "        print(i+1, a.decode('utf-8'))\n",
        "        \n",
        "    print()\n",
        "    \n",
        "    print('> Title examples:')\n",
        "    for i, t in enumerate(tit.numpy()):\n",
        "        print(i+1, t.decode('utf-8'))\n",
        "    \n",
        "    counter += 1\n",
        "    if counter == 10:\n",
        "        break"
      ],
      "metadata": {
        "id": "oMAJuyKRMJPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "6YE74HgtRlwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def title_preprocessor(text):\n",
        "    text = tf.strings.lower(text)\n",
        "    text = tf.strings.strip(text)\n",
        "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "    return text\n",
        "\n",
        "def abstract_preprocessor(text):\n",
        "    text = tf.strings.lower(text)\n",
        "    text = tf.strings.strip(text)\n",
        "    # Added start and end tokens. read somewhere its important for the RNNs\n",
        "    # not for transformer.. Idk.\n",
        "    # - Damion\n",
        "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "    return text\n",
        "\n",
        "# def abstract_preprocessor_mk2(text):\n",
        "#     # from: https://www.tensorflow.org/text/tutorials/nmt_with_attention#text_preprocessing\n",
        "#     # I've included this below code for preprocessing, since in the abstract,\n",
        "#     # punctuation etc isnt important - perhaps. Only for the titles. \n",
        "#     # So, perhaps performance increase\n",
        "#     # - Damion\n",
        "\n",
        "#     # Additionally, the start and end token seem to do something\n",
        "#     # for the RNN's?\n",
        "#     # Split accente d characters.\n",
        "    \n",
        "#     #text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "#     #text = tf.strings.lower(text)\n",
        "    \n",
        "#     # Keep space, a to z, and select punctuation.\n",
        "#     #text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "#     text = tf.strings.regex_replace(text, '[^ a-z]', '')\n",
        "    \n",
        "#     # Add spaces around punctuation.\n",
        "#     #text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "    \n",
        "#     # Strip whitespace.\n",
        "#     text = tf.strings.strip(text)\n",
        "\n",
        "#     text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "#     return text\n",
        "\n",
        "abs_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=abstract_preprocessor,\n",
        "    ragged=True,\n",
        "    output_mode='int'\n",
        "    )\n",
        "\n",
        "# abs_text_processor = tf.keras.layers.TextVectorization(\n",
        "#     standardize=abstract_preprocessor_mk2,\n",
        "#     ragged=True,\n",
        "#     output_mode='int'\n",
        "#     )\n",
        "\n",
        "tit_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=title_preprocessor,\n",
        "    ragged=True,\n",
        "    output_mode='int'\n",
        "    )\n",
        "\n",
        "abs_text_processor.adapt(train_dataset.map(lambda abs, tit: abs))\n",
        "tit_text_processor.adapt(train_dataset.map(lambda abs, tit: tit))\n",
        "\n",
        "def process_text(context, target):\n",
        "    context = abs_text_processor(context).to_tensor()\n",
        "    target = tit_text_processor(target)\n",
        "    targ_in = target[:,:-1].to_tensor()\n",
        "    targ_out = target[:,1:].to_tensor()\n",
        "    return (context, targ_in), targ_out\n",
        "\n",
        "\n",
        "train_ds = train_dataset.map(process_text, tf.data.AUTOTUNE)\n",
        "val_ds = val_dataset.map(process_text, tf.data.AUTOTUNE)\n",
        "test_ds = test_dataset.map(process_text, tf.data.AUTOTUNE)\n",
        "\n"
      ],
      "metadata": {
        "id": "LdZwW7FwMJoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for (ex_context_tok, ex_tar_in), ex_tar_out in train_ds.take(1):\n",
        "    print(ex_context_tok[0, :10].numpy()) \n",
        "    print()\n",
        "    print(ex_tar_in[0, :10].numpy()) \n",
        "    print(ex_tar_out[0, :10].numpy())"
      ],
      "metadata": {
        "id": "tWz6GNDBMJvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models + components"
      ],
      "metadata": {
        "id": "EHJAZYiwMJ1M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN + GRU components"
      ],
      "metadata": {
        "id": "i8X0JFHTMJ6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(tf.keras.layers.Layer):\n",
        "  def __init__(self, text_processor, units, GRU=False):\n",
        "    super(EncoderRNN, self).__init__()\n",
        "    self.text_processor = text_processor\n",
        "    self.vocab_size = text_processor.vocabulary_size()\n",
        "    self.units = units\n",
        "\n",
        "    # The embedding layer converts tokens to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.vocab_size, units,\n",
        "                                               mask_zero=True)\n",
        "\n",
        "    # The RNN layer processes those vectors sequentially.\n",
        "    # self.rnn = tf.keras.layers.Bidirectional(\n",
        "    #     merge_mode='sum',\n",
        "    #     layer=tf.keras.layers.GRU(units,\n",
        "    #                         # Return the sequence and state\n",
        "    #                         return_sequences=True,\n",
        "    #                         recurrent_initializer='glorot_uniform'))\n",
        "    if not GRU:\n",
        "        self.rnn = (tf.keras.layers \\\n",
        "                    .SimpleRNN(units,\n",
        "                    # Return the sequence and state\n",
        "                    return_sequences=True,\n",
        "                    recurrent_initializer='glorot_uniform'))\n",
        "    else:\n",
        "        self.rnn = (tf.keras.layers\n",
        "                    .GRU(units,\n",
        "                    # Return the sequence and state\n",
        "                    return_sequences=True,\n",
        "                    recurrent_initializer='glorot_uniform'))\n",
        "\n",
        "  def call(self, x):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(x, 'batch s')\n",
        "\n",
        "    # 2. The embedding layer looks up the embedding vector for each token.\n",
        "    x = self.embedding(x)\n",
        "    shape_checker(x, 'batch s units')\n",
        "\n",
        "    # 3. The GRU processes the sequence of embeddings.\n",
        "    x = self.rnn(x)\n",
        "    shape_checker(x, 'batch s units')\n",
        "\n",
        "    # 4. Returns the new sequence of embeddings.\n",
        "    return x\n",
        "\n",
        "  def convert_input(self, texts):\n",
        "    texts = tf.convert_to_tensor(texts)\n",
        "    if len(texts.shape) == 0:\n",
        "      texts = tf.convert_to_tensor(texts)[tf.newaxis]\n",
        "    context = self.text_processor(texts).to_tensor()\n",
        "    context = self(context)\n",
        "    return context"
      ],
      "metadata": {
        "id": "OSwUQAd0MJ_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttentionRNN(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=1, **kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "  def call(self, x, context):\n",
        "    shape_checker = ShapeChecker()\n",
        "\n",
        "    shape_checker(x, 'batch t units')\n",
        "    shape_checker(context, 'batch s units')\n",
        "\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        value=context,\n",
        "        return_attention_scores=True)\n",
        "\n",
        "    shape_checker(x, 'batch t units')\n",
        "    shape_checker(attn_scores, 'batch heads t s')\n",
        "\n",
        "    # Cache the attention scores for plotting later.\n",
        "    attn_scores = tf.reduce_mean(attn_scores, axis=1)\n",
        "    shape_checker(attn_scores, 'batch t s')\n",
        "    self.last_attention_weights = attn_scores\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "ibffKtxVR_Dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(tf.keras.layers.Layer):\n",
        "    @classmethod\n",
        "    def add_method(cls, fun):\n",
        "        setattr(cls, fun.__name__, fun)\n",
        "        return fun\n",
        "\n",
        "    def __init__(self, text_processor, units, GRU=True):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.text_processor = text_processor\n",
        "        self.vocab_size = text_processor.vocabulary_size()\n",
        "        self.word_to_id = tf.keras.layers.StringLookup(\n",
        "            vocabulary=text_processor.get_vocabulary(),\n",
        "            mask_token='', oov_token='[UNK]')\n",
        "        self.id_to_word = tf.keras.layers.StringLookup(\n",
        "            vocabulary=text_processor.get_vocabulary(),\n",
        "            mask_token='', oov_token='[UNK]',\n",
        "            invert=True)\n",
        "        self.start_token = self.word_to_id('[START]')\n",
        "        self.end_token = self.word_to_id('[END]')\n",
        "        print('Did start- and end-token get stored properly?')\n",
        "        print(self.start_token, self.end_token)\n",
        "\n",
        "        self.units = units\n",
        "\n",
        "\n",
        "        # 1. The embedding layer converts token IDs to vectors\n",
        "        self.embedding = tf.keras.layers.Embedding(self.vocab_size,\n",
        "                                                    units, mask_zero=True)\n",
        "\n",
        "        # 2. The RNN keeps track of what's been generated so far.\n",
        "        if not GRU:\n",
        "            self.rnn = (tf.keras.layers \\\n",
        "                        .SimpleRNN(units,\n",
        "                        # Return the sequence and state\n",
        "                        return_sequences=True,\n",
        "                        return_state=True,\n",
        "                        recurrent_initializer='glorot_uniform'))\n",
        "        else:\n",
        "            self.rnn = (tf.keras.layers \\\n",
        "                        .GRU(units,\n",
        "                        # Return the sequence and state\n",
        "                        return_sequences=True,\n",
        "                        return_state=True,\n",
        "                        recurrent_initializer='glorot_uniform'))\n",
        "\n",
        "        # 3. The RNN output will be the query for the attention layer.\n",
        "        self.attention = CrossAttentionRNN(units)\n",
        "\n",
        "        # 4. This fully connected layer produces the logits for each\n",
        "        # output token.\n",
        "        self.output_layer = tf.keras.layers.Dense(self.vocab_size)\n",
        "\n",
        "    def call(self,\n",
        "            context, x,\n",
        "            state=None,\n",
        "            return_state=False):  \n",
        "        shape_checker = ShapeChecker()\n",
        "        shape_checker(x, 'batch t')\n",
        "        shape_checker(context, 'batch s units')\n",
        "\n",
        "        # 1. Lookup the embeddings\n",
        "        x = self.embedding(x)\n",
        "        shape_checker(x, 'batch t units')\n",
        "\n",
        "        # 2. Process the target sequence.\n",
        "        x, state = self.rnn(x, initial_state=state)\n",
        "        shape_checker(x, 'batch t units')\n",
        "\n",
        "        # 3. Use the RNN output as the query for the attention over the context.\n",
        "        x = self.attention(x, context)\n",
        "        self.last_attention_weights = self.attention.last_attention_weights\n",
        "        shape_checker(x, 'batch t units')\n",
        "        shape_checker(self.last_attention_weights, 'batch t s')\n",
        "\n",
        "        # Step 4. Generate logit predictions for the next token.\n",
        "        logits = self.output_layer(x)\n",
        "        shape_checker(logits, 'batch t target_vocab_size')\n",
        "\n",
        "        if return_state:\n",
        "            return logits, state\n",
        "        else:\n",
        "            return logits\n",
        "\n",
        "    def get_initial_state(self, context):\n",
        "        batch_size = tf.shape(context)[0]\n",
        "        start_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "        done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "        embedded = self.embedding(start_tokens)\n",
        "        return start_tokens, done, self.rnn.get_initial_state(embedded)[0]\n",
        "    \n",
        "    def tokens_to_text(self, tokens):\n",
        "        words = self.id_to_word(tokens)\n",
        "        result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "        result = tf.strings.regex_replace(result, '^ *\\[START\\] *', '')\n",
        "        result = tf.strings.regex_replace(result, ' *\\[END\\] *$', '')\n",
        "        return result\n",
        "\n",
        "    def get_next_token(self, context, next_token, done, state, temperature = 0.0):\n",
        "        logits, state = self(\n",
        "        context, next_token,\n",
        "        state = state,\n",
        "        return_state=True) \n",
        "\n",
        "        if temperature == 0.0:\n",
        "            next_token = tf.argmax(logits, axis=-1)\n",
        "        else:\n",
        "            logits = logits[:, -1, :]/temperature\n",
        "            next_token = tf.random.categorical(logits, num_samples=1)\n",
        "\n",
        "        # If a sequence produces an `end_token`, set it `done`\n",
        "        done = done | (next_token == self.end_token)\n",
        "        # Once a sequence is done it only produces 0-padding.\n",
        "        next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n",
        "\n",
        "        return next_token, done, state"
      ],
      "metadata": {
        "id": "dUrqxE8-SENh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Translator(tf.keras.Model):\n",
        "    @classmethod\n",
        "    def add_method(cls, fun):\n",
        "        setattr(cls, fun.__name__, fun)\n",
        "        return fun\n",
        "\n",
        "    def __init__(self, units,\n",
        "                context_text_processor,\n",
        "                target_text_processor,\n",
        "                GRU=True):\n",
        "        super().__init__()\n",
        "        # Build the encoder and decoder\n",
        "        # GRU False will result in SimpleRNN instead of GRU\n",
        "        self.encoder = EncoderRNN(context_text_processor, units, GRU=GRU)\n",
        "        self.decoder = DecoderRNN(target_text_processor, units, GRU=GRU)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        context, x = inputs\n",
        "        context = self.encoder(context)\n",
        "        logits = self.decoder(context, x)\n",
        "\n",
        "        #TODO(b/250038731): remove this\n",
        "        try:\n",
        "            # Delete the keras mask, so keras doesn't scale the loss+accuracy. \n",
        "            del logits._keras_mask\n",
        "        except AttributeError:\n",
        "            pass\n",
        "\n",
        "        return logits\n",
        "    \n",
        "    def translate(self,\n",
        "                texts, *,\n",
        "                max_length=50,\n",
        "                temperature=0.0):\n",
        "        # Process the input texts\n",
        "        context = self.encoder.convert_input(texts)\n",
        "        batch_size = tf.shape(texts)[0]\n",
        "\n",
        "        # Setup the loop inputs\n",
        "        tokens = []\n",
        "        attention_weights = []\n",
        "        next_token, done, state = self.decoder.get_initial_state(context)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # Generate the next token\n",
        "            next_token, done, state = self.decoder.get_next_token(\n",
        "                context, next_token, done,  state, temperature)\n",
        "\n",
        "            # Collect the generated tokens\n",
        "            tokens.append(next_token)\n",
        "            attention_weights.append(self.decoder.last_attention_weights)\n",
        "\n",
        "            if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "                break\n",
        "\n",
        "        # Stack the lists of tokens and attention weights.\n",
        "        tokens = tf.concat(tokens, axis=-1)   # t*[(batch 1)] -> (batch, t)\n",
        "        self.last_attention_weights = tf.concat(attention_weights, axis=1)  # t*[(batch 1 s)] -> (batch, t s)\n",
        "\n",
        "        result = self.decoder.tokens_to_text(tokens)\n",
        "        return result"
      ],
      "metadata": {
        "id": "-ejJ7fX0SnbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Component"
      ],
      "metadata": {
        "id": "zdNrC7w4MKDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbeddingT(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
        "        self.pos_encoding = self.positional_encoding(length=2048, depth=d_model)\n",
        "    \n",
        "    def compute_mask(self, *args, **kwargs):\n",
        "        return self.embedding.compute_mask(*args, **kwargs)\n",
        "    \n",
        "    def call(self, x):\n",
        "        length = tf.shape(x)[1]\n",
        "        x = self.embedding(x)\n",
        "        # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "        return x\n",
        "    \n",
        "    def positional_encoding(self, length, depth):\n",
        "        depth = depth/2\n",
        "\n",
        "        positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "        depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "\n",
        "        angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "        angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "        pos_encoding = np.concatenate(\n",
        "            [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "            axis=-1) \n",
        "\n",
        "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "class BaseAttentionT(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "        self.add = tf.keras.layers.Add()\n",
        "\n",
        "class CrossAttentionT(BaseAttentionT):\n",
        "    def call(self, x, context):\n",
        "        attn_output, attn_scores = self.mha(\n",
        "            query=x,\n",
        "            key=context,\n",
        "            value=context,\n",
        "            return_attention_scores=True)\n",
        "        # Cache the attention scores for plotting later.\n",
        "        self.last_attn_scores = attn_scores\n",
        "        x = self.add([x, attn_output])\n",
        "        x = self.layernorm(x)\n",
        "        return x\n",
        "\n",
        "class GlobalSelfAttentionT(BaseAttentionT):\n",
        "    def call(self, x):\n",
        "        attn_output = self.mha(\n",
        "            query=x,\n",
        "            value=x,\n",
        "            key=x)\n",
        "        x = self.add([x, attn_output])\n",
        "        x = self.layernorm(x)\n",
        "        return x\n",
        "\n",
        "class CausalSelfAttentionT(BaseAttentionT):\n",
        "    def call(self, x):\n",
        "        attn_output = self.mha(\n",
        "            query=x,\n",
        "            value=x,\n",
        "            key=x,\n",
        "            use_causal_mask = True)\n",
        "        x = self.add([x, attn_output])\n",
        "        x = self.layernorm(x)\n",
        "        return x\n",
        "\n",
        "class FeedForwardT(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.seq = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(dff, activation='relu'),\n",
        "            tf.keras.layers.Dense(d_model),\n",
        "            tf.keras.layers.Dropout(dropout_rate)\n",
        "        ])\n",
        "        self.add = tf.keras.layers.Add()\n",
        "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "    def call(self, x):\n",
        "        x = self.add([x, self.seq(x)])\n",
        "        x = self.layer_norm(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "F_NNvdiJMKIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayerT(tf.keras.layers.Layer):\n",
        "    def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attention = GlobalSelfAttentionT(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=d_model,\n",
        "            dropout=dropout_rate)\n",
        "\n",
        "        self.ffn = FeedForwardT(d_model, dff)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.self_attention(x)\n",
        "        x = self.ffn(x)\n",
        "        return x\n",
        "\n",
        "class EncoderT(tf.keras.layers.Layer):\n",
        "    def __init__(self, *, num_layers, d_model, num_heads,\n",
        "                dff, vocab_size, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Wanneer we translator maken moeten we het volgende gebruiken:\n",
        "        # self.text_processor = text_processor\n",
        "        # self.vocab_size = vocab_size\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.pos_embedding = PositionalEmbeddingT(\n",
        "            vocab_size=vocab_size, d_model=d_model)\n",
        "\n",
        "        self.enc_layers = [\n",
        "            EncoderLayerT(d_model=d_model,\n",
        "                            num_heads=num_heads,\n",
        "                            dff=dff,\n",
        "                            dropout_rate=dropout_rate)\n",
        "            for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x):\n",
        "        # `x` is token-IDs shape: (batch, seq_len)\n",
        "        x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "\n",
        "        # Add dropout.\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x)\n",
        "\n",
        "        return x  # Shape `(batch_size, seq_len, d_model)`.\n",
        "\n",
        "    def convert_input(self, texts):\n",
        "        pass"
      ],
      "metadata": {
        "id": "he59iwfiTOf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayerT(tf.keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "               *,\n",
        "               d_model,\n",
        "               num_heads,\n",
        "               dff,\n",
        "               dropout_rate=0.1):\n",
        "    super(DecoderLayerT, self).__init__()\n",
        "\n",
        "    self.causal_self_attention = CausalSelfAttentionT(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "    \n",
        "    self.cross_attention = CrossAttentionT(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForwardT(d_model, dff)\n",
        "\n",
        "  def call(self, x, context):\n",
        "    x = self.causal_self_attention(x=x)\n",
        "    x = self.cross_attention(x=x, context=context)\n",
        "\n",
        "    # Cache the last attention scores for plotting later\n",
        "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
        "\n",
        "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "    return x\n",
        "\n",
        "class DecoderT(tf.keras.layers.Layer):\n",
        "    def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
        "                dropout_rate=0.1):\n",
        "        super(DecoderT, self).__init__()\n",
        "        ### TODO:\n",
        "        # word_to_id etc, zie RNN decoder van Nick en Jan\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.pos_embedding = PositionalEmbeddingT(vocab_size=vocab_size,\n",
        "                                                    d_model=d_model)\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.dec_layers = [\n",
        "            DecoderLayerT(d_model=d_model, num_heads=num_heads,\n",
        "                            dff=dff, dropout_rate=dropout_rate)\n",
        "            for _ in range(num_layers)]\n",
        "\n",
        "        self.last_attn_scores = None\n",
        "\n",
        "    def call(self, x, context):\n",
        "        # `x` is token-IDs shape (batch, target_seq_len)\n",
        "        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x  = self.dec_layers[i](x, context)\n",
        "\n",
        "        self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
        "\n",
        "        # The shape of x is (batch_size, target_seq_len, d_model).\n",
        "        return x"
      ],
      "metadata": {
        "id": "P8_myjAATnb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
        "                input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = EncoderT(num_layers=num_layers, d_model=d_model,\n",
        "                                num_heads=num_heads, dff=dff,\n",
        "                                vocab_size=input_vocab_size,\n",
        "                                dropout_rate=dropout_rate)\n",
        "\n",
        "        self.decoder = DecoderT(num_layers=num_layers, d_model=d_model,\n",
        "                                num_heads=num_heads, dff=dff,\n",
        "                                vocab_size=target_vocab_size,\n",
        "                                dropout_rate=dropout_rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        # To use a Keras model with `.fit` you must pass all your inputs in the\n",
        "        # first argument.\n",
        "        context, x = inputs\n",
        "\n",
        "        context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
        "\n",
        "        x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
        "\n",
        "        # Final linear layer output.\n",
        "        logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
        "\n",
        "        try:\n",
        "            # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
        "            # b/250038731\n",
        "            del logits._keras_mask\n",
        "        except AttributeError:\n",
        "            pass\n",
        "\n",
        "        # Return the final output and the attention weights.\n",
        "        return logits"
      ],
      "metadata": {
        "id": "X3F7avH3Tnqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "gE75RbueMKMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training functions"
      ],
      "metadata": {
        "id": "QZzYkh4BMKQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_loss(label, pred):\n",
        "    mask = label != 0\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "    loss = loss_object(label, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss *= mask\n",
        "\n",
        "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def masked_accuracy(label, pred):\n",
        "    pred = tf.argmax(pred, axis=2)\n",
        "    label = tf.cast(label, pred.dtype)\n",
        "    match = label == pred\n",
        "\n",
        "    mask = label != 0\n",
        "\n",
        "    match = match & mask\n",
        "\n",
        "    match = tf.cast(match, dtype=tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "tJPDgRnQMKVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Used for transformer\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    step = tf.cast(step, dtype=tf.float32)\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "  \n",
        "  def get_config(self):\n",
        "      config = {\n",
        "      'd_model': self.d_model,\n",
        "      'warmup_steps': self.warmup_steps,\n",
        "\n",
        "        }\n",
        "      return config"
      ],
      "metadata": {
        "id": "KOAN3jYDMKZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models setup (with hyper-pars)"
      ],
      "metadata": {
        "id": "KkbRafPUMKw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN\n",
        "RNN_UNITS = 256\n",
        "modelRNN = Translator(RNN_UNITS, abs_text_processor, tit_text_processor, GRU=False)\n"
      ],
      "metadata": {
        "id": "Jtyvjc7uVDNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRU\n",
        "GRU_UNITS = 256\n",
        "modelGRU = Translator(GRU_UNITS, abs_text_processor, tit_text_processor, GRU=True)"
      ],
      "metadata": {
        "id": "9xL9yyf-VurD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer hyper pars\n",
        "num_layers = 1\n",
        "d_model = 128\n",
        "dff = 256\n",
        "num_heads = 4 # was 8\n",
        "dropout_rate = 0.1\n",
        "\n",
        "# Training pars\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
        "                                     epsilon=1e-9)\n",
        "\n",
        "# Load model\n",
        "modelT = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=len(abs_text_processor.get_vocabulary()),\n",
        "    target_vocab_size=len(tit_text_processor.get_vocabulary()),\n",
        "    dropout_rate=dropout_rate)"
      ],
      "metadata": {
        "id": "5JOx9eLFVwBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compile"
      ],
      "metadata": {
        "id": "K7ZeY0yuWd4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelGRU.compile(\n",
        "    loss=masked_loss,\n",
        "    optimizer='Adam',\n",
        "    metrics=[masked_accuracy])"
      ],
      "metadata": {
        "id": "AWXO-6XOVDU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelRNN.compile(\n",
        "    loss=masked_loss,\n",
        "    optimizer='Adam',\n",
        "    metrics=[masked_accuracy])"
      ],
      "metadata": {
        "id": "JbYQVdOvVDex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelT.compile(\n",
        "    loss=masked_loss,\n",
        "    optimizer=optimizer,\n",
        "    metrics=[masked_accuracy])"
      ],
      "metadata": {
        "id": "B-qpqkWmVDiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train RNN"
      ],
      "metadata": {
        "id": "TW8e8ez8VDm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "    historyRNN = modelRNN.fit(train_ds,\n",
        "                            epochs=30,\n",
        "                            validation_data=val_ds,\n",
        "                            callbacks=tf.keras.callbacks.EarlyStopping(\n",
        "                                monitor='val_loss',\n",
        "                                patience=3))"
      ],
      "metadata": {
        "id": "pPtY-POqVDqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train GRU"
      ],
      "metadata": {
        "id": "JC2vwjx8VDvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "    historyGRU = modelGRU.fit(train_ds,\n",
        "                            epochs=30,\n",
        "                            validation_data=val_ds,\n",
        "                            callbacks=tf.keras.callbacks.EarlyStopping(\n",
        "                                monitor='val_loss',\n",
        "                                patience=3))"
      ],
      "metadata": {
        "id": "uv5avzTeVDxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Transformer"
      ],
      "metadata": {
        "id": "3-MoQyFAVD2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "    historyT = modelT.fit(train_ds,\n",
        "                            epochs=30,\n",
        "                            validation_data=val_ds,\n",
        "                            callbacks=tf.keras.callbacks.EarlyStopping(\n",
        "                                monitor='val_loss',\n",
        "                                patience=3))"
      ],
      "metadata": {
        "id": "hz1cUqc9VD5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training summary"
      ],
      "metadata": {
        "id": "NMSoPdJ-VD-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history, model_name):\n",
        "    # Acc's\n",
        "    plt.plot(history.history['masked_accuracy'])\n",
        "    plt.plot(history.history['val_masked_accuracy'])\n",
        "    plt.title(f'model accuracy - {model_name}')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "    plt.show()\n",
        "    \n",
        "    # summarize history for loss\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title(f'model loss - {model_name}')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "518YdRRIVECE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN"
      ],
      "metadata": {
        "id": "hJji4JmeXXvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelRNN.summary()"
      ],
      "metadata": {
        "id": "_hJzotmKXIZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(historyRNN, f'RNN ({RNN_UNITS})')"
      ],
      "metadata": {
        "id": "thGXlE1iXIow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRU"
      ],
      "metadata": {
        "id": "T5mzL7-NXXLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelGRU.summary()"
      ],
      "metadata": {
        "id": "fypT0ix6XIte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(historyGRU, f'GRU ({GRU_UNITS})')"
      ],
      "metadata": {
        "id": "eO-8LA35XIzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer"
      ],
      "metadata": {
        "id": "TaKCfdFsXWKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelT.summary()"
      ],
      "metadata": {
        "id": "sRrzXLb7XWPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for naming\n",
        "pars = dict(L = num_layers, emb = d_model, ff = dff, heads = num_heads)\n",
        "\n",
        "s = ''\n",
        "for k, v in pars:\n",
        "    s += f'{k}={v}; '\n",
        "s = s[:-2]\n",
        "\n",
        "# plot history\n",
        "plot_history(historyGRU, f'Transformer ({s})')"
      ],
      "metadata": {
        "id": "XHCgyzq7XWoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference\n",
        "\n",
        "1. Add inference from both branches on git, as implemented. Adjust variable names with above.\n",
        "2. Use same sample from test-set\n",
        "\n"
      ],
      "metadata": {
        "id": "-hCKJt_PXWuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_translation(abs, pred, ground_truth):\n",
        "    print(f'{\"Input:\":15s}: {abs}')\n",
        "    print(f'{\"Prediction\":15s}: {pred}')\n",
        "    print(f'{\"Ground truth\":15s}: {ground_truth}')"
      ],
      "metadata": {
        "id": "qeXwd4JmCgqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = random.sample(range(1087), 6)\n",
        "print(samples)\n",
        "rnn_x1, rnn_y1 = test_text[samples[0]], test_labels[samples[0]]\n",
        "rnn_x2, rnn_y2 = test_text[samples[1]], test_labels[samples[1]]\n",
        "gru_x1, gru_y1 = test_text[samples[2]], test_labels[samples[2]]\n",
        "gru_x2, gru_y2 = test_text[samples[3]], test_labels[samples[3]]\n",
        "tr_x1, tr_y1 = test_text[samples[4]], test_labels[samples[4]]\n",
        "tr_x2, tr_y2 = test_text[samples[5]], test_labels[samples[5]]"
      ],
      "metadata": {
        "id": "yuGtF2cQD2vY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = \"we propose a solution to the annotation bottleneck for statistical parsing , by exploiting the lexicalized nature of combinatory categorial grammar ( ccg ) . the parsing model uses predicate-argument dependencies for training , which are derived from sequences of ccg lexical categories rather than full derivations . a simple method is used for extracting dependencies from lexical category sequences , resulting in high precision , yet incomplete and noisy data . the dependency parsing model of clark and curran ( 2004b ) is extended to exploit this partial training data . remarkably , the accuracy of the parser trained on data derived from category sequences alone is only 1.3 % worse in terms of f-score than the parser trained on complete dependency structures .\"\n",
        "x2 = \"this paper describes a probabilistic model for coordination disambiguation integrated into syntactic and case structure analysis . our model probabilistically assesses the parallelism of a candidate coordinate structure using syntactic/semantic similarities and cooccurrence statistics . we integrate these probabilities into the framework of fully-lexicalized parsing based on largescale case frames . this approach simultaneously addresses two tasks of coordination disambiguation : the detection of coordinate conjunctions and the scope disambiguation of coordinate structures . experimental results on web sentences indicate the effectiveness of our approach .\"\n",
        "y1 = \"partial training for a lexicalized-grammar parser\"\n",
        "y2 = \"probabilistic coordination disambiguation in a fully-lexicalized japanese parser\""
      ],
      "metadata": {
        "id": "_P3pMIxcEWop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN"
      ],
      "metadata": {
        "id": "-ruIbY49YfKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_yhat1 = modelRNN.translate([x1])\n",
        "rnn_yhat2 = modelRNN.translate([x2])\n",
        "\n",
        "print_translation(x1, rnn_yhat1[0].numpy().decode(), y1)\n",
        "print_translation(x2, rnn_yhat2[0].numpy().decode(), y2)"
      ],
      "metadata": {
        "id": "4QVH8z1wYgpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRU"
      ],
      "metadata": {
        "id": "RX2SIIsyYg9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gru_yhat1 = modelGRU.translate([x1])\n",
        "gru_yhat2 = modelGRU.translate([x2])\n",
        "\n",
        "print_translation(x1, gru_yhat1[0].numpy().decode(), y1)\n",
        "print_translation(x2, gru_yhat2[0].numpy().decode(), y2)"
      ],
      "metadata": {
        "id": "UYIFwbO3YhCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer"
      ],
      "metadata": {
        "id": "mOtapooZYhPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslatorT(tf.Module):\n",
        "  def __init__(self, tokenizer, detokenizer, transformer):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.detokenizer = detokenizer\n",
        "    self.transformer = transformer\n",
        "\n",
        "  def __call__(self, sentence):\n",
        "    # The input sentence is an abstract, hence adding the `[START]` and `[END]` tokens.\n",
        "    assert isinstance(sentence, tf.Tensor)\n",
        "    if len(sentence.shape) == 0:\n",
        "      sentence = sentence[tf.newaxis]\n",
        "\n",
        "    # TODO: use our own tokenizer\n",
        "    sentence = self.tokenizer(sentence).to_tensor()\n",
        "\n",
        "    encoder_input = sentence\n",
        "\n",
        "    # As the output language a title, initialize the output with the\n",
        "    # `[START]` token.\n",
        "    # TODO: look up how to tokenize this\n",
        "    start_end = self.detokenizer([''])[0]\n",
        "    start = start_end[0][tf.newaxis]\n",
        "    end = start_end[1][tf.newaxis]\n",
        "\n",
        "    # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
        "    # dynamic-loop can be traced by `tf.function`.\n",
        "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
        "    output_array = output_array.write(0, start)\n",
        "\n",
        "    # 50 is an arbitrary break-point\n",
        "    for i in tf.range(50):\n",
        "      output = tf.transpose(output_array.stack())\n",
        "      predictions = self.transformer([encoder_input, output], training=False)\n",
        "\n",
        "      # Select the last token from the `seq_len` dimension.\n",
        "      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
        "\n",
        "      predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "      # Concatenate the `predicted_id` to the output which is given to the\n",
        "      # decoder as its input.\n",
        "      output_array = output_array.write(i+1, predicted_id[0])\n",
        "\n",
        "      if predicted_id == end:\n",
        "        break\n",
        "\n",
        "    output = tf.transpose(output_array.stack())\n",
        "\n",
        "    tit_vocab = np.array(self.detokenizer.get_vocabulary())\n",
        "    tokens = tit_vocab[output.numpy()]\n",
        "    text = [' '.join(tok) for tok in tokens]\n",
        "\n",
        "    # The output shape is `(1, tokens)`.\n",
        "    # text = self.detokenizer(output)[0]  # Shape: `()`.\n",
        "\n",
        "    #tokens = self.detokenizer lookup(output)[0]\n",
        "\n",
        "    # `tf.function` prevents us from using the attention_weights that were\n",
        "    # calculated on the last iteration of the loop.\n",
        "    # So, recalculate them outside the loop.\n",
        "    self.transformer([encoder_input, output[:,:-1]], training=False)\n",
        "    attention_weights = self.transformer.decoder.last_attn_scores\n",
        "\n",
        "    # return text, tokens, attention_weights\n",
        "    return text, attention_weights\n"
      ],
      "metadata": {
        "id": "tk8d_jb1YknO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transT = TranslatorT(abs_text_processor, tit_text_processor, modelT)"
      ],
      "metadata": {
        "id": "xg_tKuY8-ZvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_yhat1, _ = transT(tf.constant(x1))\n",
        "t_yhat2, _ = transT(tf.constant(x2))\n",
        "\n",
        "t_yhat1 = t_yhat1[0].replace('[START] ', '').replace(' [END]', '')\n",
        "t_yhat2 = t_yhat2[0].replace('[START] ', '').replace(' [END]', '')\n",
        "\n",
        "print_translation(x1, t_yhat1, y1)\n",
        "print_translation(x2, t_yhat2, y2)"
      ],
      "metadata": {
        "id": "5CpTksIG9YKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BertScore\n",
        "\n",
        "See code snippit Samuel on teams"
      ],
      "metadata": {
        "id": "jXsR9llQYlpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/Tiiiger/bert_score\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "kP8d2HgEYlty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bert_score import BERTScorer\n",
        "import logging\n",
        "import transformers"
      ],
      "metadata": {
        "id": "YCVDzIbrYlx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)"
      ],
      "metadata": {
        "id": "o9p64fnCYl2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity checking:\n",
        "generated_titles = [\n",
        "    'example generated title 1',\n",
        "    'example generated title 2',\n",
        "    'example generated title 3'\n",
        "    ]\n",
        "\n",
        "actual_titles = [\n",
        "    'example actual title 1',\n",
        "    'example actual title 2',\n",
        "    'example actual title 3'\n",
        "    ]\n",
        "\n",
        "# Precision, Recall, F1 scores \n",
        "# len(actual_titles) == len(generated_titles) == len(P) == len(R) == len(F1)\n",
        "P, R, F1 = scorer.score(generated_titles, actual_titles)"
      ],
      "metadata": {
        "id": "dIVmevkbYl6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_preds, gru_preds, tra_preds = [], [], []\n",
        "counter = 0\n",
        "for text in test_text:\n",
        "    rnn_preds.append(modelRNN.translate([text])[0].numpy().decode())\n",
        "    gru_preds.append(modelGRU.translate([text])[0].numpy().decode())\n",
        "    \n",
        "    tp, _ = transT(tf.constant(text))\n",
        "    tp = tp[0].replace('[START] ', '').replace(' [END]', '')\n",
        "    \n",
        "    tra_preds.append(tp)\n",
        "\n",
        "    counter+=1\n",
        "    if counter % 100 == 0:\n",
        "        print(f'{counter/len(test_labels)}%')"
      ],
      "metadata": {
        "id": "ON8mjTNrFDZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_P, rnn_R, rnn_F1 = scorer.score(rnn_preds, test_labels)"
      ],
      "metadata": {
        "id": "TEWyzegfFX2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gru_P, gru_R, gru_F1 = scorer.score(gru_preds, test_labels)"
      ],
      "metadata": {
        "id": "iSML9ydLFdmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tra_P, tra_R, tra_F1 = scorer.score(tra_preds, test_labels)"
      ],
      "metadata": {
        "id": "lRiUoPUQFelF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"RNN\\t\\tP mean: {},\\tR mean: {},\\tF1 mean: {}\".format(rnn_P.mean(), rnn_R.mean(), rnn_F1.mean()))\n",
        "print(\"GRU\\t\\tP mean: {},\\tR mean: {},\\tF1 mean: {}\".format(gru_P.mean(), gru_R.mean(), gru_F1.mean()))\n",
        "print(\"Transformer\\tP mean: {},\\tR mean: {},\\tF1 mean: {}\".format(tra_P.mean(), tra_R.mean(), tra_F1.mean()))"
      ],
      "metadata": {
        "id": "iOx360WnTKmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"RNN\\t\\tP std: {},\\tR std: {},\\tF1 std: {}\".format(rnn_P.std(), rnn_R.std(), rnn_F1.std()))\n",
        "print(\"GRU\\t\\tP std: {},\\tR std: {},\\tF1 std: {}\".format(gru_P.std(), gru_R.std(), gru_F1.std()))\n",
        "print(\"Transformer\\tP std: {},\\tR std: {},\\tF1 std: {}\".format(tra_P.std(), tra_R.std(), tra_F1.std()))"
      ],
      "metadata": {
        "id": "RNuTEqnDTK_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best performers\n",
        "\n",
        "Below shows the top-10 best performing titles per model."
      ],
      "metadata": {
        "id": "uQV2RHr1Yaut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_topk(header, vals, idxs, preds):\n",
        "  print(\"{}\".format(header))\n",
        "  [print_top(idx, v, test_text, test_labels, preds) for v, idx in zip(vals, idxs)]\n",
        "\n",
        "def print_top(idx, v, inps, lbl, prds):\n",
        "  print(\"Idx {} ({})\".format(idx, v))\n",
        "  print(\"True: {}\\nPred: {}\".format(lbl[idx], prds[idx]))\n",
        "  print(\"Abst: {}\\n\".format(inps[idx]))"
      ],
      "metadata": {
        "id": "AzlIX7w1cpi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN"
      ],
      "metadata": {
        "id": "mFWfVitcfTKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vals, idxs = rnn_F1.topk(10)\n",
        "print_topk(\"RNN\", vals, idxs, tra_preds)"
      ],
      "metadata": {
        "id": "FyFsK92LfXLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRU"
      ],
      "metadata": {
        "id": "5p0cgXDtfS4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vals, idxs = gru_F1.topk(10)\n",
        "print_topk(\"GRU\", vals, idxs, tra_preds)"
      ],
      "metadata": {
        "id": "JtcZQQj0fX26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer"
      ],
      "metadata": {
        "id": "hC9moDW6fBMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vals, idxs = tra_F1.topk(10)\n",
        "print_topk(\"Transformer\", vals, idxs, tra_preds)"
      ],
      "metadata": {
        "id": "6pJcC8bnZGX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Histograms"
      ],
      "metadata": {
        "id": "Eka5gUz6gO--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(rnn_F1)\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "ZKC3nytBgLfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(gru_F1)\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "C5DovFxVgUxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(tra_F1)\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "2zI5mMiwVJdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save data"
      ],
      "metadata": {
        "id": "28Uz2pFygj9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "yyDdX_nfhBw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(gru_P, \"rnn_P.pt\")\n",
        "torch.save(gru_P, \"gru_P.pt\")\n",
        "torch.save(gru_P, \"tra_P.pt\")"
      ],
      "metadata": {
        "id": "he8hgZ3Cgo6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(gru_R, \"rnn_R.pt\")\n",
        "torch.save(gru_R, \"gru_R.pt\")\n",
        "torch.save(gru_R, \"tra_R.pt\")"
      ],
      "metadata": {
        "id": "q57KRKOxhi0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(gru_F1, \"rnn_F1.pt\")\n",
        "torch.save(gru_F1, \"gru_F1.pt\")\n",
        "torch.save(gru_F1, \"tra_F1.pt\")"
      ],
      "metadata": {
        "id": "Q7WPQJeqgmaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(rnn_preds, \"rnn_preds.pt\")\n",
        "torch.save(gru_preds, \"gru_preds.pt\")\n",
        "torch.save(tra_preds, \"tra_preds.pt\")"
      ],
      "metadata": {
        "id": "IAnqDS_hhn4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelRNN.save(\"rnn.model\")"
      ],
      "metadata": {
        "id": "GUqIO-I5h95W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelGRU.save(\"gru.model\")"
      ],
      "metadata": {
        "id": "3nGujAusiv3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelT.save(\"transformer.model\")"
      ],
      "metadata": {
        "id": "o-NuHQ8_jH4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zip for download\n",
        "\n",
        "Cannot download dirs from colab, so zip contents first, then download the archive."
      ],
      "metadata": {
        "id": "eguuTt2OjRI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil"
      ],
      "metadata": {
        "id": "xg3DCADzkF0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.make_archive('content', 'zip', '.')"
      ],
      "metadata": {
        "id": "pViSvtfykIZE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
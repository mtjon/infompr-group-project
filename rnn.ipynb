{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mtjon/infompr-group-project/blob/feature%2Frnn/rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nma_JWh-W-IF"
      },
      "source": [
        "# Title Generation through Abstract Summarisation with RNNs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the most re version of TensorFlow to use the improved\n",
        "# masking support for `tf.keras.layers.MultiHeadAttention`.\n",
        "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n",
        "!pip uninstall -y -q tensorflow keras tensorflow-estimator tensorflow-text\n",
        "!pip install -q tensorflow_datasets\n",
        "!pip install -q -U tensorflow-text==2.9.* tensorflow==2.9.*"
      ],
      "metadata": {
        "id": "A_vHnSLfBZo2",
        "outputId": "f88908f2-df94-467d-f548-a0500fe4d2ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following packages will be REMOVED:\n",
            "  libcudnn8-dev\n",
            "The following held packages will be changed:\n",
            "  libcudnn8\n",
            "The following packages will be DOWNGRADED:\n",
            "  libcudnn8\n",
            "0 upgraded, 0 newly installed, 1 downgraded, 1 to remove and 19 not upgraded.\n",
            "Need to get 430 MB of archives.\n",
            "After this operation, 1,392 MB disk space will be freed.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libcudnn8 8.1.0.77-1+cuda11.2 [430 MB]\n",
            "Fetched 430 MB in 13s (34.4 MB/s)\n",
            "(Reading database ... 124016 files and directories currently installed.)\n",
            "Removing libcudnn8-dev (8.1.1.33-1+cuda11.2) ...\n",
            "update-alternatives: removing manually selected alternative - switching libcudnn to auto mode\n",
            "\u001b[1mdpkg:\u001b[0m \u001b[1;33mwarning:\u001b[0m downgrading libcudnn8 from 8.1.1.33-1+cuda11.2 to 8.1.0.77-1+cuda11.2\n",
            "(Reading database ... 123993 files and directories currently installed.)\n",
            "Preparing to unpack .../libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb ...\n",
            "Unpacking libcudnn8 (8.1.0.77-1+cuda11.2) over (8.1.1.33-1+cuda11.2) ...\n",
            "Setting up libcudnn8 (8.1.0.77-1+cuda11.2) ...\n",
            "\u001b[33mWARNING: Skipping tensorflow-text as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.8/511.8 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 KB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gJr_9dXGpJ05"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_text\n",
        "import tensorflow_text as tf_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-gE-Ez1qtyIA"
      },
      "outputs": [],
      "source": [
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get data\n",
        "target_url = \"https://raw.githubusercontent.com/EagleW/ACL_titles_abstracts_dataset/master/acl_titles_and_abstracts.txt\"\n",
        "response = requests.get(target_url)\n",
        "raw_data = response.text\n",
        "print(len(raw_data))\n",
        "\n",
        "with open('acl_titles_and_abstract.txt', 'w') as file:\n",
        "    file.writelines(raw_data)\n"
      ],
      "metadata": {
        "id": "NfxDR9Gt_0ix",
        "outputId": "4ca99b64-3d52-4650-f8b5-1445203375f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8678084\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "C4HZx7Gndbrh"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from random import shuffle\n",
        "# Seed voor herhaalbaarheid\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines = []\n",
        "\n",
        "# Code van https://github.com/EagleW/Writing-editing-Network/blob/master/split_data.py\n",
        "with open(\"acl_titles_and_abstract.txt\", 'r') as file1:\n",
        "  lines=file1.readlines()\n",
        "\n",
        "abs_t = []\n",
        "abstracts = []\n",
        "titles = []\n",
        "i = 0\n",
        "# TODO: possibly generates wrong tibs/abs when encountering a blank line; remove all blank lines\n",
        "for line in lines:\n",
        "    if i % 3 == 0:\n",
        "        titles.append(line)\n",
        "    elif i % 3 == 1:\n",
        "        abstracts.append(line)\n",
        "    i += 1\n",
        "for i in range(len(abstracts)):\n",
        "    if len(titles[i]) > 0 and len(abstracts[i]) > 0:\n",
        "        h_a_pair = (titles[i], abstracts[i])\n",
        "        abs_t.append(h_a_pair)\n",
        "shuffle(abs_t)\n",
        "total = len(abs_t)\n",
        "dev = total//10\n",
        "train  = total - dev - dev\n",
        "i = 0\n",
        "\n",
        "with open(\"val.txt\", 'w') as valFile:\n",
        "  for i in range(dev):\n",
        "      valFile.writelines(abs_t[i][0])\n",
        "      valFile.writelines(abs_t[i][1])\n",
        "  #    valFile.writelines(\"\\n\")\n",
        "\n",
        "with open(\"test.txt\", 'w') as testFile:\n",
        "  for i in range(dev, 2 * dev):\n",
        "      testFile.writelines(abs_t[i][0])\n",
        "      testFile.writelines(abs_t[i][1])\n",
        "  #    file1.writelines(\"\\n\")\n",
        "\n",
        "with open(\"train.txt\", 'w') as trainFile:\n",
        "  for i in range(2 * dev, total):\n",
        "      trainFile.writelines(abs_t[i][0])\n",
        "      trainFile.writelines(abs_t[i][1])\n",
        "  #    trainFile.writelines(\"\\n\")\n"
      ],
      "metadata": {
        "id": "qnJKrpDSADgw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_abs_and_titles_from_raw(path_to_file):\n",
        "    abstracts, titles = [], []\n",
        "    with open(path_to_file) as data:\n",
        "        lines = data.readlines()\n",
        "        for abs in lines[1::2]:\n",
        "            abstracts.append(abs.strip())\n",
        "        for title in lines[0::2]:\n",
        "            # // TODO: check if we need to add start and end tokens\n",
        "            titles.append(title.strip())\n",
        "            # titles.append('[START] ' + title.strip() + ' [END]')\n",
        "    return abstracts, titles"
      ],
      "metadata": {
        "id": "39UNm84FHK5Z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text, train_labels = get_abs_and_titles_from_raw('train.txt')\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_text, train_labels))\n",
        "# ((train_text, train_inputs), train_labels) <-- tokenized"
      ],
      "metadata": {
        "id": "IufsyM7CHt_i"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for abs, tit in train_dataset.batch(3).take(1):\n",
        "    print('> Abstract examples:')\n",
        "    for i, a in enumerate(abs.numpy()):\n",
        "        print(i+1, a.decode('utf-8'))\n",
        "        \n",
        "    print()\n",
        "    \n",
        "    print('> Title examples:')\n",
        "    for i, t in enumerate(tit.numpy()):\n",
        "        print(i+1, t.decode('utf-8'))\n"
      ],
      "metadata": {
        "id": "mzLBGVwDJ5hC",
        "outputId": "2dddf758-f0b1-4e3f-ae32-f04240bb5647",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Abstract examples:\n",
            "1 in this paper , we describe our participation in the tempeval-3 challenge . with our multilingual temporal tagger heideltime , we addressed task a , the extraction and normalization of temporal expressions for english and spanish . exploiting heideltimes strict separation between source code and languagedependent parts , we tuned heideltimes existing english resources and developed new spanish resources . for both languages , we achieved the best results among all participants for task a , the combination of extraction and normalization . both the improved english and the new spanish resources are publicly available with heideltime .\n",
            "2 we present three novel methods of compactly storing very large n-gram language models . these methods use substantially less space than all known approaches and allow n-gram probabilities or counts to be retrieved in constant time , at speeds comparable to modern language modeling toolkits . our basic approach generates an explicit minimal perfect hash function , that maps all n-grams in a model to distinct integers to enable storage of associated values . extensions of this approach exploit distributional characteristics of n-gram data to reduce storage costs , including variable length coding of values and the use of tiered structures that partition the data for more efficient storage . we apply our approach to storing the full google web1t n-gram set and all 1-to-5 grams of the gigaword newswire corpus . for the 1.5 billion n-grams of gigaword , for example , we can store full count information at a cost of 1.66 bytes per n-gram ( around 30 % of the cost when using the current stateof-the-art approach ) , or quantized counts for 1.41 bytes per n-gram . for applications that are tolerant of a certain class of relatively innocuous errors ( where unseen n-grams may be accepted as rare n-grams ) , we can reduce the latter cost to below 1 byte per n-gram .\n",
            "3 a poll consists of a question and a set of predefined answers from which voters can select . we present the new problem of vote prediction on comments , which involves determining which of these answers a voter selected given a comment she wrote after voting . to address this task , we exploit not only the information extracted from the comments but also extra-textual information such as user demographic information and inter-comment constraints . in an evaluation involving nearly one million comments collected from the popular sodahead social polling website , we show that a vote prediction system that exploits only textual information can be improved significantly when extended with extra-textual information .\n",
            "\n",
            "> Title examples:\n",
            "1 heideltime : tuning english and developing spanish resources\n",
            "2 storing the web in memory : space efficient language models with constant time retrieval\n",
            "3 vote prediction on comments in social polls\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.tensorflow.org/tutorials/load_data/text#example_1_predict_the_tag_for_a_stack_overflow_question\n",
        "VOCAB_SIZE = 10000\n",
        "MAX_SEQUENCE_LENGTH = 250\n",
        "\n",
        "# int_vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "#     max_tokens=VOCAB_SIZE,\n",
        "#     output_mode='int',\n",
        "#     output_sequence_length=MAX_SEQUENCE_LENGTH\n",
        "#     )\n",
        "\n",
        "int_vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "    output_mode='int'\n",
        "    )\n",
        "\n",
        "int_vectorize_layer.adapt(train_dataset)\n",
        "print(len(int_vectorize_layer.get_vocabulary()))\n"
      ],
      "metadata": {
        "id": "sv9eKCJpJ6ER",
        "outputId": "c1c5cbdf-a3e8-4cf7-88c8-61aa61784e16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.tensorflow.org/tutorials/load_data/text#prepare_the_dataset_for_training\n",
        "def int_vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return int_vectorize_layer(text), label\n",
        "\n",
        "def tokenize(text):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return int_vectorize_layer(text)"
      ],
      "metadata": {
        "id": "EusdLzuIK2I1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve a batch (of 32 abstracts and titles) from the dataset.\n",
        "text_batch, label_batch = next(iter(train_dataset.batch(32)))\n",
        "first_abstract, first_label = text_batch[0], label_batch[0]\n"
      ],
      "metadata": {
        "id": "RhJV6mg-K7FY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "v-x8BU8oLN1D"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = 'train.txt'\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "vocab = sorted(set(text))\n",
        "\n",
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "6-PvstfpPAC3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "\n",
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n"
      ],
      "metadata": {
        "id": "lJr8P415LVnu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = train_dataset\n",
        "\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
      ],
      "metadata": {
        "id": "-mxorSKBMFci",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "outputId": "f5762106-543d-414c-ba37-78c32cb59dc2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnimplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-4dc74637f323>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minput_example_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_example_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mexample_batch_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_example_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_batch_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"# (batch_size, sequence_length, vocab_size)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-b0880e9cbdeb>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, states, return_state, training)\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_initial_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnimplementedError\u001b[0m: Exception encountered when calling layer \"embedding\" (type Embedding).\n\nCast string to int32 is not supported [Op:Cast]\n\nCall arguments received by layer \"embedding\" (type Embedding):\n  • inputs=tf.Tensor(shape=(), dtype=string)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WdD7mDuaPZXl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
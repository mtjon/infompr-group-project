{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mtjon/infompr-group-project/blob/feature%2Frnn/rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nma_JWh-W-IF"
      },
      "source": [
        "# Title Generation through Abstract Summarisation with RNNs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0N0RTnvaqU4P"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_vHnSLfBZo2",
        "outputId": "6d86d0ef-219a-4551-f68d-1434477c7a7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following packages will be REMOVED:\n",
            "  libcudnn8-dev\n",
            "The following held packages will be changed:\n",
            "  libcudnn8\n",
            "The following packages will be DOWNGRADED:\n",
            "  libcudnn8\n",
            "0 upgraded, 0 newly installed, 1 downgraded, 1 to remove and 21 not upgraded.\n",
            "Need to get 430 MB of archives.\n",
            "After this operation, 1,392 MB disk space will be freed.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  libcudnn8 8.1.0.77-1+cuda11.2 [430 MB]\n",
            "Fetched 430 MB in 5s (84.5 MB/s)\n",
            "(Reading database ... 129504 files and directories currently installed.)\n",
            "Removing libcudnn8-dev (8.1.1.33-1+cuda11.2) ...\n",
            "update-alternatives: removing manually selected alternative - switching libcudnn to auto mode\n",
            "\u001b[1mdpkg:\u001b[0m \u001b[1;33mwarning:\u001b[0m downgrading libcudnn8 from 8.1.1.33-1+cuda11.2 to 8.1.0.77-1+cuda11.2\n",
            "(Reading database ... 129481 files and directories currently installed.)\n",
            "Preparing to unpack .../libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb ...\n",
            "Unpacking libcudnn8 (8.1.0.77-1+cuda11.2) over (8.1.1.33-1+cuda11.2) ...\n",
            "Setting up libcudnn8 (8.1.0.77-1+cuda11.2) ...\n",
            "\u001b[33mWARNING: Skipping tensorflow-text as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.8/511.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 KB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.0\n"
          ]
        }
      ],
      "source": [
        "# Install the most re version of TensorFlow to use the improved\n",
        "# masking support for `tf.keras.layers.MultiHeadAttention`.\n",
        "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n",
        "!pip uninstall -y -q tensorflow keras tensorflow-estimator tensorflow-text\n",
        "!pip install -q tensorflow_datasets\n",
        "!pip install -q -U tensorflow-text==2.9.* tensorflow==2.9.*\n",
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gJr_9dXGpJ05"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_text\n",
        "import tensorflow_text as tf_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-gE-Ez1qtyIA"
      },
      "outputs": [],
      "source": [
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data import"
      ],
      "metadata": {
        "id": "YoJ2dCVo9Gtu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NfxDR9Gt_0ix"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from random import shuffle\n",
        "# Seed voor herhaalbaarheid\n",
        "random.seed(42)\n",
        "\n",
        "TRAIN_PCT = 70\n",
        "TEST_PCT = 25\n",
        "VAL_PCT = 5\n",
        "\n",
        "TOTAL_PCT = TRAIN_PCT + TEST_PCT + VAL_PCT\n",
        "\n",
        "# modified data externally to remove all blank lines\n",
        "def write_data(f, ext, mod):\n",
        "    # Code van https://github.com/EagleW/Writing-editing-Network/blob/master/split_data.py\n",
        "    file1=open(f, 'r')\n",
        "    lines=file1.readlines()\n",
        "    file1.close()\n",
        "    abs_t = []\n",
        "    abstracts = []\n",
        "    titles = []\n",
        "    i = 0\n",
        "    # TODO: possibly generates wrong tibs/abs when encounter\n",
        "    for line in lines:\n",
        "        if i % mod == 0:\n",
        "            titles.append(line)\n",
        "        elif i % mod == 1:\n",
        "            abstracts.append(line)\n",
        "        i += 1\n",
        "    for i in range(len(abstracts)):\n",
        "        if len(titles[i]) > 0 and len(abstracts[i]) > 0:\n",
        "            h_a_pair = (titles[i], abstracts[i])\n",
        "            abs_t.append(h_a_pair)\n",
        "\n",
        "    GRANULARITY_FACTOR = 100\n",
        "\n",
        "    shuffle(abs_t)\n",
        "    total_lines = len(abs_t)\n",
        "    train_lines = (((total_lines * GRANULARITY_FACTOR) // TOTAL_PCT) * TRAIN_PCT) // GRANULARITY_FACTOR\n",
        "    test_lines = (((total_lines * GRANULARITY_FACTOR) // TOTAL_PCT) * TEST_PCT) // GRANULARITY_FACTOR\n",
        "    val_lines = total_lines - (train_lines + test_lines) # different calculation to ensure no lines get left out\n",
        "\n",
        "    i = 0\n",
        "    with open(\"val{}.txt\".format(ext), 'w') as file1:\n",
        "      for i in range(train_lines):\n",
        "          file1.writelines(abs_t[i][0])\n",
        "          file1.writelines(abs_t[i][1])\n",
        "      #    file1.writelines(\"\\n\")\n",
        "\n",
        "    with open(\"test{}.txt\".format(ext), 'w') as file1:\n",
        "      for i in range(train_lines, train_lines+test_lines):\n",
        "          file1.writelines(abs_t[i][0])\n",
        "          file1.writelines(abs_t[i][1])\n",
        "      #    file1.writelines(\"\\n\")\n",
        "\n",
        "    with open(\"train{}.txt\".format(ext), 'w') as file1:\n",
        "      for i in range(train_lines+test_lines, total_lines):\n",
        "          file1.writelines(abs_t[i][0])\n",
        "          file1.writelines(abs_t[i][1])\n",
        "      #    file1.writelines(\"\\n\")\n",
        "\n",
        "    print(\"Split data into (train, test, validate) as follows: \", (train_lines, test_lines, val_lines), \"from total lines:\", total_lines)\n",
        "\n",
        "\n",
        "def get_abs_and_titles_from_raw(path_to_file):\n",
        "    abstracts, titles = [], []\n",
        "    with open(path_to_file) as data:\n",
        "        lines = data.readlines()\n",
        "        for abs in lines[1::2]:\n",
        "            abstracts.append(abs.strip())\n",
        "        for title in lines[0::2]:\n",
        "            # // TODO: check if we need to add start and end tokens\n",
        "            titles.append(title.strip())\n",
        "            # titles.append('[START] ' + title.strip() + ' [END]')\n",
        "    return abstracts, titles\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary when upload fails (Firefox+Colab issue)\n",
        "\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "metadata": {
        "id": "1cdZ-lfi6w1y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "78f00176-a855-4641-cfcc-d6e03cea2fe8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8df6c87e-b9b3-4a0d-a04a-bb04c8f9b3b4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8df6c87e-b9b3-4a0d-a04a-bb04c8f9b3b4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving acl_titles_and_abstracts_mod.txt to acl_titles_and_abstracts_mod.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39UNm84FHK5Z",
        "outputId": "5ab6dfba-a865-432a-85e2-31b4d5643e4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split data into (train, test, validate) as follows:  (7612, 2718, 545) from total lines: 10875\n"
          ]
        }
      ],
      "source": [
        "write_data(\"acl_titles_and_abstracts_mod.txt\", \"\", 2) # writes train, val, test.txt\n",
        "\n",
        "train_text, train_labels = get_abs_and_titles_from_raw('train.txt')\n",
        "val_text, val_labels = get_abs_and_titles_from_raw('val.txt')\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_text, train_labels)).batch(16)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_text, val_labels)).batch(16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mzLBGVwDJ5hC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4651fb13-e626-4ae7-f23f-4643b69c3c0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Abstract examples:\n",
            "1 the computation of meaning similarity as operationalized by vector-based models has found widespread use in many tasks ranging from the acquisition of synonyms and paraphrases to word sense disambiguation and textual entailment . vector-based models are typically directed at representing words in isolation and thus best suited for measuring similarity out of context . in his paper we propose a probabilistic framework for measuring similarity in context . central to our approach is the intuition that word meaning is represented as a probability distribution over a set of latent senses and is modulated by context . experimental results on lexical substitution and word similarity show that our algorithm outperforms previously proposed models .\n",
            "2 this paper ties up some loose ends in finite-state optimality theory . first , it discusses how to perform comprehension under optimality theory grammars consisting of finite-state constraints . comprehension has not been much studied in ot ; we show that unlike production , it does not always yield a regular set , making finite-state methods inapplicable . however , after giving a suitably flexible presentation of ot , we show carefully how to treat comprehension under recent variants of ot in which grammars can be compiled into finite-state transducers . we then unify these variants , showing that compilation is possible if all components of the grammar are regular relations , including the harmony ordering on scored candidates . a side benefit of our construction is a far simpler implementation of directional ot ( eisner , 2000 ) .\n",
            "3 code-switched documents are common in social media , providing evidence for polylingual topic models to infer aligned topics across languages . we present code-switched lda ( cslda ) , which infers language specific topic distributions based on code-switched documents to facilitate multi-lingual corpus analysis . we experiment on two code-switching corpora ( english-spanish twitter data and english-chinese weibo data ) and show that cslda improves perplexity over lda , and learns semantically coherent aligned topics as judged by human annotators .\n",
            "4 in cases in which there is no standard orthography for a language or language variant , written texts will display a variety of orthographic choices . this is problematic for natural language processing ( nlp ) because it creates spurious data sparseness . we study the transformation of spontaneously spelled egyptian arabic into a conventionalized orthography which we have previously proposed for nlp purposes . we show that a two-stage process can reduce divergences from this standard by 69 % , making subsequent processing of egyptian arabic easier .\n",
            "5 this paper describes a system designed to disambiguate person names in a set of web pages . in our approach web documents are represented as different sets of features or terms of different types ( bag of words , urls , names and numbers ) . we apply agglomerative vector space clustering that uses the similarity between pairs of analogous feature sets . this system achieved a value of 66 % for f=0.2 and a value of 48 % for f=0.5 in the web people search task at semeval-2007 .\n",
            "6 we introduce a social media text normalization system that can be deployed as a preprocessing step for machine translation and various nlp applications to handle social media text . the proposed system is based on unsupervised learning of the normalization equivalences from unlabeled text . the proposed approach uses random walks on a contextual similarity bipartite graph constructed from n-gram sequences on large unlabeled text corpus . we show that the proposed approach has a very high precision of ( 92.43 ) and a reasonable recall of ( 56.4 ) . when used as a preprocessing step for a state-of-the-art machine translation system , the translation quality on social media text improved by 6 % . the proposed approach is domain and language independent and can be deployed as a preprocessing step for any nlp application to handle social media text .\n",
            "7 information extraction ( ie ) systems assist analysts to assimilate information from electronic documents . this paper focuses on ie tasks designed to support information discovery applications . since information discovery implies examining large volumes of documents drawn from various sources for situations that can not be anticipated a priori , they require ie systems to have breadth as well as depth . this implies the need for a domain-independent ie system that can easily be customized for specific domains : end users must be given tools to customize the system on their own . it also implies the need for defining new intermediate level ie tasks that are richer than the subject-verb-object ( svo ) triples produced by shallow systems , yet not as complex as the domain-specific scenarios defined by the message understanding conference ( muc ) . this paper describes a robust , scalable ie engine designed for such purposes . it describes new ie tasks such as entity profiles , and concept-based general events which represent realistic goals in terms of what can be accomplished in the near-term as well as providing useful , actionable information . these new tasks also facilitate the correlation of output from an ie engine with existing structured data . benchmarking results for the core engine and applications utilizing the engine are presented .\n",
            "8 in this paper we investigate the efficiency of the novel term weighting algorithm for opinion mining and topic categorization of articles from newspapers and internet . we compare the novel term weighting technique with existing approaches such as tf-idf and confweight . the performance on the data from the text-mining campaigns deft07 and deft08 shows that the proposed method can compete with existing information retrieval models in classification quality and that it is computationally faster . the proposed text preprocessing method can be applied in large-scale information retrieval and data mining problems and it can be easily transported to different domains and different languages since it does not require any domain-related or linguistic information .\n",
            "9 frequency counts from very large corpora , such as the web 1t dataset , have recently become available for language modeling . omission of low frequency n-gram counts is a practical necessity for datasets of this size . naive implementations of standard smoothing methods do not realize the full potential of such large datasets with missing counts . in this paper i present a new smoothing algorithm that combines the dirichlet prior form of ( mackay and peto , 1995 ) with the modified back-off estimates of ( kneser and ney , 1995 ) that leads to a 31 % perplexity reduction on the brown corpus compared to a baseline implementation of kneser-ney discounting .\n",
            "10 we introduce several ideas that improve the performance of supervised information extraction systems with a pipeline architecture , when they are customized for new domains . we show that : ( a ) a combination of a sequence tagger with a rule-based approach for entity mention extraction yields better performance for both entity and relation mention extraction ; ( b ) improving the identification of syntactic heads of entity mentions helps relation extraction ; and ( c ) a deterministic inference engine captures some of the joint domain structure , even when introduced as a postprocessing step to a pipeline system . all in all , our contributions yield a 20 % relative increase in f1 score in a domain significantly different from the domains used during the development of our information extraction system .\n",
            "11 this paper presents an approach to the question whether it is possible to construct a parser based on ideas from case-based reasoning . such a parser would employ a partial analysis of the input sentence to select a ( nearly ) complete syntax tree and then adapt this tree to the input sentence . the experiments performed on german data from the tuba-d/z treebank and the karopars partial parser show that a wide range of levels of generality can be reached , depending on which types of information are used to determine the similarity between input sentence and training sentences . the results are such that it is possible to construct a case-based parser . the optimal setting out of those presented here need to be determined empirically .\n",
            "12 in modern biology , digitization of biosystematics publications is an important task . extraction of taxonomic names from such documents is one of its major issues . this is because these names identify the various genera and species . this article reports on our experiences with learning techniques for this particular task . we say why established named-entity recognition techniques are somewhat difficult to use in our context . one reason is that we have only very little training data available . our experiments show that a combining approach that relies on regular expressions , heuristics , and word-level language recognition achieves very high precision and recall and allows to cope with those difficulties .\n",
            "13 sentence compression has been shown to benefit from joint inference involving both n-gram and dependency-factored objectives but this typically requires expensive integer programming . we explore instead the use of lagrangian relaxation to decouple the two subproblems and solve them separately . while dynamic programming is viable for bigram-based sentence compression , finding optimal compressed trees within graphs is np-hard . we recover approximate solutions to this problem using lp relaxation and maximum spanning tree algorithms , yielding techniques that can be combined with the efficient bigrambased inference approach using lagrange multipliers . experiments show that these approximation strategies produce results comparable to a state-of-the-art integer linear programming formulation for the same joint inference task along with a significant improvement in runtime .\n",
            "14 we approached the problem as learning how to order documents by estimated relevance with respect to a user query . our support vector machines based classifier learns from the relevance judgments available with the standard test collections and generalizes to new , previously unseen queries . for this , we have designed a representation scheme , which is based on the discrete representation of the local ( lw ) and global ( gw ) weighting functions , thus is capable of reproducing and enhancing the properties of such popular ranking functions as tf.idf , bm25 or those based on language models . our tests with the standard test collections have demonstrated the capability of our approach to achieve the performance of the best known scoring functions solely from the labeled examples and without taking advantage of knowing those functions or their important properties or parameters .\n",
            "15 this paper presents a method for automatic topic identification using a graph-centrality algorithm applied to an encyclopedic graph derived from wikipedia . when tested on a data set with manually assigned topics , the system is found to significantly improve over a simpler baseline that does not make use of the external encyclopedic knowledge .\n",
            "16 a lack of standard datasets and evaluation metrics has prevented the field of paraphrasing from making the kind of rapid progress enjoyed by the machine translation community over the last 15 years . we address both problems by presenting a novel data collection framework that produces highly parallel text data relatively inexpensively and on a large scale . the highly parallel nature of this data allows us to use simple n-gram comparisons to measure both the semantic adequacy and lexical dissimilarity of paraphrase candidates . in addition to being simple and efficient to compute , experiments show that these metrics correlate highly with human judgments .\n",
            "\n",
            "> Title examples:\n",
            "1 measuring distributional similarity in context\n",
            "2 comprehension and compilation in optimality theory\n",
            "3 learning polylingual topic models from code-switched social media documents\n",
            "4 processing spontaneous orthography\n",
            "5 composition of simple bags of typed terms escuela politcnica superior legans ( madrid ) spain\n",
            "6 social text normalization using contextual graph random walks\n",
            "7 infoxtract : a customizable intermediate level information\n",
            "8 opinion mining and topic categorization with novel term weighting\n",
            "9 smoothing a tera-word language model\n",
            "10 customizing an information extraction system to a new domain\n",
            "11 towards case-based parsing : are chunks reliable indicators for syntax trees\n",
            "12 the difficulties of taxonomic name extraction and a solution guido sautter klemens bhm\n",
            "13 approximation strategies for multi-structure sentence compression\n",
            "14 discretization based learning approach to information retrieval dmitri roussinov weiguo fan\n",
            "15 topic identification using wikipedia graph centrality\n",
            "16 collecting highly parallel data for paraphrase evaluation\n"
          ]
        }
      ],
      "source": [
        "for abs, tit in train_dataset.take(1):\n",
        "    print('> Abstract examples:')\n",
        "    for i, a in enumerate(abs.numpy()):\n",
        "        print(i+1, a.decode('utf-8'))\n",
        "        \n",
        "    print()\n",
        "    \n",
        "    print('> Title examples:')\n",
        "    for i, t in enumerate(tit.numpy()):\n",
        "        print(i+1, t.decode('utf-8'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "styje2jU9gAS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "sv9eKCJpJ6ER"
      },
      "outputs": [],
      "source": [
        "def title_preprocessor(text):\n",
        "    text = tf.strings.lower(text)\n",
        "    text = tf.strings.strip(text)\n",
        "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "    return text\n",
        "\n",
        "def abstract_preprocessor(text):\n",
        "    text = tf.strings.lower(text)\n",
        "    text = tf.strings.strip(text)\n",
        "    return text\n",
        "\n",
        "abs_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=abstract_preprocessor,\n",
        "    ragged=True,\n",
        "    output_mode='int'\n",
        "    )\n",
        "\n",
        "tit_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=title_preprocessor,\n",
        "    ragged=True,\n",
        "    output_mode='int'\n",
        "    )\n",
        "\n",
        "abs_text_processor.adapt(train_dataset.map(lambda abs, tit: abs))\n",
        "tit_text_processor.adapt(train_dataset.map(lambda abs, tit: tit))\n",
        "\n",
        "\n",
        "\n",
        "def process_text(context, target):\n",
        "    context = abs_text_processor(context).to_tensor()\n",
        "    target = tit_text_processor(target)\n",
        "    targ_in = target[:,:-1].to_tensor()\n",
        "    targ_out = target[:,1:].to_tensor()\n",
        "    return (context, targ_in), targ_out\n",
        "\n",
        "\n",
        "train_ds = train_dataset.map(process_text, tf.data.AUTOTUNE)\n",
        "val_ds = val_dataset.map(process_text, tf.data.AUTOTUNE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import einops\n",
        "\n",
        "#@title\n",
        "class ShapeChecker():\n",
        "  def __init__(self):\n",
        "    # Keep a cache of every axis-name seen\n",
        "    self.shapes = {}\n",
        "\n",
        "  def __call__(self, tensor, names, broadcast=False):\n",
        "    if not tf.executing_eagerly():\n",
        "      return\n",
        "\n",
        "    parsed = einops.parse_shape(tensor, names)\n",
        "\n",
        "    for name, new_dim in parsed.items():\n",
        "      old_dim = self.shapes.get(name, None)\n",
        "      \n",
        "      if (broadcast and new_dim == 1):\n",
        "        continue\n",
        "\n",
        "      if old_dim is None:\n",
        "        # If the axis name is new, add its length to the cache.\n",
        "        self.shapes[name] = new_dim\n",
        "        continue\n",
        "\n",
        "      if new_dim != old_dim:\n",
        "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
        "                         f\"    found: {new_dim}\\n\"\n",
        "                         f\"    expected: {old_dim}\\n\")"
      ],
      "metadata": {
        "id": "QVwvbAJKQdjf"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder"
      ],
      "metadata": {
        "id": "qHDtLLx99kgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConfigurableEncoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, text_processor, units, embedding, rnn):\n",
        "    super(ConfigurableEncoder, self).__init__()\n",
        "    self.text_processor = text_processor\n",
        "    self.vocab_size = text_processor.vocabulary_size()\n",
        "    self.units = units\n",
        "    \n",
        "    # The embedding layer converts tokens to vectors\n",
        "    self.embedding = embedding\n",
        "\n",
        "    # The RNN layer processes those vectors sequentially.\n",
        "    self.rnn = rnn\n",
        "\n",
        "  def call(self, x):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(x, 'batch s')\n",
        "\n",
        "    # 2. The embedding layer looks up the embedding vector for each token.\n",
        "    x = self.embedding(x)\n",
        "    shape_checker(x, 'batch s units')\n",
        "\n",
        "    # 3. The GRU processes the sequence of embeddings.\n",
        "    x = self.rnn(x)\n",
        "    shape_checker(x, 'batch s units')\n",
        "\n",
        "    # 4. Returns the new sequence of embeddings.\n",
        "    return x\n",
        "\n",
        "  def convert_input(self, texts):\n",
        "    texts = tf.convert_to_tensor(texts)\n",
        "    if len(texts.shape) == 0:\n",
        "      texts = tf.convert_to_tensor(texts)[tf.newaxis]\n",
        "    context = self.text_processor(texts).to_tensor()\n",
        "    context = self(context)\n",
        "    return context\n",
        "\n",
        "  def configuration(self):\n",
        "    return {\n",
        "        \"embedding\": str(self.embedding),\n",
        "        \"rnn\": str(self.rnn)\n",
        "    }"
      ],
      "metadata": {
        "id": "U_V0rCn5N5E8"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder definitions"
      ],
      "metadata": {
        "id": "G-fhDnfdZEiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend\n",
        "\n",
        "class MinimalRNNCell(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, units, **kwargs):\n",
        "        self.units = units\n",
        "        self.state_size = units\n",
        "        super(MinimalRNNCell, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                                      initializer='uniform',\n",
        "                                      name='kernel')\n",
        "        self.recurrent_kernel = self.add_weight(\n",
        "            shape=(self.units, self.units),\n",
        "            initializer='uniform',\n",
        "            name='recurrent_kernel')\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, states):\n",
        "        prev_output = states[0]\n",
        "        h = backend.dot(inputs, self.kernel)\n",
        "        output = h + backend.dot(prev_output, self.recurrent_kernel)\n",
        "        return output, [output]\n",
        "\n",
        "\n",
        "def unidirectional_encoder(text_processor, units):\n",
        "  embedding = tf.keras.layers.Embedding(text_processor.vocabulary_size(), \n",
        "                                        units,\n",
        "                                        mask_zero=True)\n",
        "  rnn = tf.keras.layers.RNN(cell=MinimalRNNCell(32))\n",
        "\n",
        "  return ConfigurableEncoder(text_processor, units, embedding, rnn)  \n",
        "\n",
        "def bidirectional_encoder(text_processor, units, merge_mode='sum'):\n",
        "  embedding = tf.keras.layers.Embedding(text_processor.vocabulary_size(), \n",
        "                                        units,\n",
        "                                        mask_zero=True)\n",
        "  rnn_layer = tf.keras.layers.GRU(units,\n",
        "                                  # Return the sequence and state\n",
        "                                  return_sequences=True,\n",
        "                                  recurrent_initializer='glorot_uniform')\n",
        "  rnn = tf.keras.layers.Bidirectional(merge_mode=merge_mode, layer=rnn_layer)\n",
        "\n",
        "  return ConfigurableEncoder(text_processor, units, embedding, rnn)  "
      ],
      "metadata": {
        "id": "10QqEfk_YQKM"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "UNITS = 256\n",
        "\n",
        "for (ex_context_tok, ex_tar_in), ex_tar_out in train_ds.take(1):\n",
        "  print(ex_context_tok[0, :10].numpy()) \n",
        "  print()\n",
        "  print(ex_tar_in[0, :10].numpy()) \n",
        "  print(ex_tar_out[0, :10].numpy()) \n",
        "\n",
        "# Encode the input sequence.\n",
        "encoder = unidirectional_encoder(abs_text_processor, UNITS)\n",
        "ex_context = encoder(ex_context_tok)\n",
        "\n",
        "print(f'Context tokens, shape (batch, s): {ex_context_tok.shape}')\n",
        "print(f'Encoder output, shape (batch, s, units): {ex_context.shape}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "id": "W4sMP96CUkV4",
        "outputId": "c18a93f5-1a2b-4864-81f9-a7a4c99b2c08"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   2 1856    5  349  116   19 4431   20 2499   53]\n",
            "\n",
            "[  2 216 130  40   8 104   0   0   0   0]\n",
            "[216 130  40   8 104   3   0   0   0   0]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-9e3ca2724602>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Encode the input sequence.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munidirectional_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs_text_processor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUNITS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mex_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex_context_tok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Context tokens, shape (batch, s): {ex_context_tok.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-91b1da420796>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# 3. The GRU processes the sequence of embeddings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mshape_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch s units'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# 4. Returns the new sequence of embeddings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-81-70834688566a>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, tensor, names, broadcast)\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meinops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_dim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/einops/einops.py\u001b[0m in \u001b[0;36mparse_shape\u001b[0;34m(x, pattern)\u001b[0m\n\u001b[1;32m    571\u001b[0m                     pattern=pattern, shape=shape))\n\u001b[1;32m    572\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m             raise RuntimeError(\"Can't parse shape with different number of dimensions: {pattern} {shape}\".format(\n\u001b[0m\u001b[1;32m    574\u001b[0m                 pattern=pattern, shape=shape))\n\u001b[1;32m    575\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_ellipsis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Exception encountered when calling layer \"configurable_encoder_12\" (type ConfigurableEncoder).\n\nCan't parse shape with different number of dimensions: batch s units (16, 32)\n\nCall arguments received by layer \"configurable_encoder_12\" (type ConfigurableEncoder):\n  • x=tf.Tensor(shape=(16, 223), dtype=int64)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention layer"
      ],
      "metadata": {
        "id": "T5Gq9dBB93BZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWKtdJ4PIY9L"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=1, **kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "  def call(self, x, context):\n",
        "    shape_checker = ShapeChecker()\n",
        " \n",
        "    shape_checker(x, 'batch t units')\n",
        "    shape_checker(context, 'batch s units')\n",
        "\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        value=context,\n",
        "        return_attention_scores=True)\n",
        "    \n",
        "    shape_checker(x, 'batch t units')\n",
        "    shape_checker(attn_scores, 'batch heads t s')\n",
        "    \n",
        "    # Cache the attention scores for plotting later.\n",
        "    attn_scores = tf.reduce_mean(attn_scores, axis=1)\n",
        "    shape_checker(attn_scores, 'batch t s')\n",
        "    self.last_attention_weights = attn_scores\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "attention_layer = CrossAttention(UNITS)\n",
        "\n",
        "# Attend to the encoded tokens\n",
        "embed = tf.keras.layers.Embedding(tit_text_processor.vocabulary_size(),\n",
        "                                  output_dim=UNITS, mask_zero=True)\n",
        "ex_tar_embed = embed(ex_tar_in)\n",
        "\n",
        "result = attention_layer(ex_tar_embed, ex_context)\n",
        "\n",
        "print(f'Context sequence, shape (batch, s, units): {ex_context.shape}')\n",
        "print(f'Target sequence, shape (batch, t, units): {ex_tar_embed.shape}')\n",
        "print(f'Attention result, shape (batch, t, units): {result.shape}')\n",
        "print(f'Attention weights, shape (batch, t, s):    {attention_layer.last_attention_weights.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder"
      ],
      "metadata": {
        "id": "bWUQDQc195ks"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmyB2Y4wIlud"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  @classmethod\n",
        "  def add_method(cls, fun):\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return fun\n",
        "\n",
        "  def __init__(self, text_processor, units):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.text_processor = text_processor\n",
        "    self.vocab_size = text_processor.vocabulary_size()\n",
        "    self.word_to_id = tf.keras.layers.StringLookup(\n",
        "        vocabulary=text_processor.get_vocabulary(),\n",
        "        mask_token='', oov_token='[UNK]')\n",
        "    self.id_to_word = tf.keras.layers.StringLookup(\n",
        "        vocabulary=text_processor.get_vocabulary(),\n",
        "        mask_token='', oov_token='[UNK]',\n",
        "        invert=True)\n",
        "    self.start_token = self.word_to_id('[START]')\n",
        "    self.end_token = self.word_to_id('[END]')\n",
        "\n",
        "    self.units = units\n",
        "\n",
        "\n",
        "    # 1. The embedding layer converts token IDs to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.vocab_size,\n",
        "                                               units, mask_zero=True)\n",
        "\n",
        "    # 2. The RNN keeps track of what's been generated so far.\n",
        "    self.rnn = tf.keras.layers.GRU(units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    # 3. The RNN output will be the query for the attention layer.\n",
        "    self.attention = CrossAttention(units)\n",
        "\n",
        "    # 4. This fully connected layer produces the logits for each\n",
        "    # output token.\n",
        "    self.output_layer = tf.keras.layers.Dense(self.vocab_size)\n",
        "\n",
        "\n",
        "@Decoder.add_method\n",
        "def call(self,\n",
        "         context, x,\n",
        "         state=None,\n",
        "         return_state=False):  \n",
        "  shape_checker = ShapeChecker()\n",
        "  shape_checker(x, 'batch t')\n",
        "  shape_checker(context, 'batch s units')\n",
        "\n",
        "  # 1. Lookup the embeddings\n",
        "  x = self.embedding(x)\n",
        "  shape_checker(x, 'batch t units')\n",
        "\n",
        "  # 2. Process the target sequence.\n",
        "  x, state = self.rnn(x, initial_state=state)\n",
        "  shape_checker(x, 'batch t units')\n",
        "\n",
        "  # 3. Use the RNN output as the query for the attention over the context.\n",
        "  x = self.attention(x, context)\n",
        "  self.last_attention_weights = self.attention.last_attention_weights\n",
        "  shape_checker(x, 'batch t units')\n",
        "  shape_checker(self.last_attention_weights, 'batch t s')\n",
        "\n",
        "  # Step 4. Generate logit predictions for the next token.\n",
        "  logits = self.output_layer(x)\n",
        "  shape_checker(logits, 'batch t target_vocab_size')\n",
        "\n",
        "  if return_state:\n",
        "    return logits, state\n",
        "  else:\n",
        "    return logits\n",
        "\n",
        "decoder = Decoder(tit_text_processor, UNITS)\n",
        "logits = decoder(ex_context, ex_tar_in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEvEyuBiIvR_"
      },
      "outputs": [],
      "source": [
        "@Decoder.add_method\n",
        "def get_initial_state(self, context):\n",
        "  batch_size = tf.shape(context)[0]\n",
        "  start_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "  embedded = self.embedding(start_tokens)\n",
        "  return start_tokens, done, self.rnn.get_initial_state(embedded)[0]\n",
        "\n",
        "\n",
        "@Decoder.add_method\n",
        "def tokens_to_text(self, tokens):\n",
        "  words = self.id_to_word(tokens)\n",
        "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "  result = tf.strings.regex_replace(result, '^ *\\[START\\] *', '')\n",
        "  result = tf.strings.regex_replace(result, ' *\\[END\\] *$', '')\n",
        "  return result\n",
        "\n",
        "\n",
        "@Decoder.add_method\n",
        "def get_next_token(self, context, next_token, done, state, temperature = 0.0):\n",
        "  logits, state = self(\n",
        "    context, next_token,\n",
        "    state = state,\n",
        "    return_state=True) \n",
        "  \n",
        "  if temperature == 0.0:\n",
        "    next_token = tf.argmax(logits, axis=-1)\n",
        "  else:\n",
        "    logits = logits[:, -1, :]/temperature\n",
        "    next_token = tf.random.categorical(logits, num_samples=1)\n",
        "\n",
        "  # If a sequence produces an `end_token`, set it `done`\n",
        "  done = done | (next_token == self.end_token)\n",
        "  # Once a sequence is done it only produces 0-padding.\n",
        "  next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n",
        "  \n",
        "  return next_token, done, state\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJfnv6diIv9e"
      },
      "outputs": [],
      "source": [
        "# Generation loop\n",
        "\n",
        "# Setup the loop variables.\n",
        "next_token, done, state = decoder.get_initial_state(ex_context)\n",
        "tokens = []\n",
        "\n",
        "for n in range(10):\n",
        "  # Run one step.\n",
        "  next_token, done, state = decoder.get_next_token(\n",
        "      ex_context, next_token, done, state, temperature=1.0)\n",
        "  # Add the token to the output.\n",
        "  tokens.append(next_token)\n",
        "\n",
        "# Stack all the tokens together.\n",
        "tokens = tf.concat(tokens, axis=-1) # (batch, t)\n",
        "\n",
        "# Convert the tokens back to a a string\n",
        "result = decoder.tokens_to_text(tokens)\n",
        "result[:3].numpy()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarisation Model\n",
        "\n",
        "The model includes an encoder and a decoder as defined above\n"
      ],
      "metadata": {
        "id": "S5JYO7HC9_Wb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EusdLzuIK2I1"
      },
      "outputs": [],
      "source": [
        "class SummarisationModel(tf.keras.Model):\n",
        "  @classmethod\n",
        "  def add_method(cls, fun):\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return fun\n",
        "\n",
        "  def __init__(self, units,\n",
        "               context_text_processor,\n",
        "               target_text_processor):\n",
        "    super().__init__()\n",
        "    # Build the encoder and decoder\n",
        "    encoder = bidirectional_encoder(context_text_processor, units)\n",
        "    decoder = Decoder(target_text_processor, units)\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def call(self, inputs):\n",
        "    context, x = inputs\n",
        "    context = self.encoder(context)\n",
        "    logits = self.decoder(context, x)\n",
        "\n",
        "    #TODO(b/250038731): remove this\n",
        "    try:\n",
        "      # Delete the keras mask, so keras doesn't scale the loss+accuracy. \n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    return logits\n",
        "  \n",
        "  def configuration(self):\n",
        "    return {\n",
        "        \"encoder\": self.encoder.configuration(),\n",
        "        \"decoder\": self.decoder.configuration()\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model creation and training"
      ],
      "metadata": {
        "id": "oUwJB2NH-FZR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhJV6mg-K7FY"
      },
      "outputs": [],
      "source": [
        "model = SummarisationModel(UNITS, abs_text_processor, tit_text_processor)\n",
        "\n",
        "logits = model((ex_context_tok, ex_tar_in))\n",
        "\n",
        "print(f'Context tokens, shape: (batch, s, units) {ex_context_tok.shape}')\n",
        "print(f'Target tokens, shape: (batch, t) {ex_tar_in.shape}')\n",
        "print(f'logits, shape: (batch, t, target_vocabulary_size) {logits.shape}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-x8BU8oLN1D"
      },
      "outputs": [],
      "source": [
        "def masked_loss(label, pred):\n",
        "    mask = label != 0\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "    loss = loss_object(label, pred)\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss *= mask\n",
        "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "    return loss\n",
        "def masked_acc(label, pred):\n",
        "    pred = tf.argmax(pred, axis=2)\n",
        "    label = tf.cast(label, pred.dtype)\n",
        "    match = label == pred\n",
        "    mask = label != 0\n",
        "    match = match & mask\n",
        "    match = tf.cast(match, dtype=tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    return tf.reduce_sum(match)/tf.reduce_sum(mask)\n",
        "model.compile(\n",
        "    loss=masked_loss,\n",
        "    optimizer='adam',\n",
        "    metrics=[masked_acc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-PvstfpPAC3"
      },
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "model.compile(optimizer='adam',\n",
        "              loss=masked_loss, \n",
        "              metrics=[masked_acc, masked_loss])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIJLxTLdJPts"
      },
      "outputs": [],
      "source": [
        "vocab_size = 1.0 * tit_text_processor.vocabulary_size()\n",
        "\n",
        "{\"expected_loss\": tf.math.log(vocab_size).numpy(),\n",
        " \"expected_acc\": 1/vocab_size}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.evaluate(val_ds, steps=20, return_dict=True)"
      ],
      "metadata": {
        "id": "0-TazbuZWptd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLQEOBueJRnL"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_ds.repeat(), \n",
        "    epochs=100,\n",
        "    steps_per_epoch = 100,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=[ tf.keras.callbacks.EarlyStopping(patience=3) ]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results"
      ],
      "metadata": {
        "id": "yDjEJGEN-Ku6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(val_ds, steps=20, return_dict=True)"
      ],
      "metadata": {
        "id": "CLQJ6KTbWHTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.configuration()"
      ],
      "metadata": {
        "id": "wSUgQOVDjd0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdD7mDuaPZXl"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('CE/token')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['masked_acc'], label='masked_acc')\n",
        "plt.plot(history.history['val_masked_acc'], label='val_masked_acc')\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "-BRzQvABg_RV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pVOG-0hThDKN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
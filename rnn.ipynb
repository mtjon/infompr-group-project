{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mtjon/infompr-group-project/blob/feature%2Frnn/rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nma_JWh-W-IF"
      },
      "source": [
        "# Title Generation through Abstract Summarisation with RNNs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the most re version of TensorFlow to use the improved\n",
        "# masking support for `tf.keras.layers.MultiHeadAttention`.\n",
        "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n",
        "!pip uninstall -y -q tensorflow keras tensorflow-estimator tensorflow-text\n",
        "!pip install -q tensorflow_datasets\n",
        "!pip install -q -U tensorflow-text==2.9.* tensorflow==2.9.*"
      ],
      "metadata": {
        "id": "A_vHnSLfBZo2",
        "outputId": "25531380-a046-4114-fa64-ce7a3193ff4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libcudnn8 is already the newest version (8.1.0.77-1+cuda11.2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 19 not upgraded.\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.8/511.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 KB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "gJr_9dXGpJ05"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_text\n",
        "import tensorflow_text as tf_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-gE-Ez1qtyIA"
      },
      "outputs": [],
      "source": [
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get data\n",
        "target_url = \"https://raw.githubusercontent.com/EagleW/ACL_titles_abstracts_dataset/master/acl_titles_and_abstracts.txt\"\n",
        "response = requests.get(target_url)\n",
        "raw_data = response.text\n",
        "print(len(raw_data))\n",
        "\n",
        "with open('acl_titles_and_abstract.txt', 'w') as file:\n",
        "    file.writelines(raw_data)\n"
      ],
      "metadata": {
        "id": "NfxDR9Gt_0ix",
        "outputId": "ab52b42f-70d3-44ca-d147-3f6140bde89b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8678084\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "C4HZx7Gndbrh"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from random import shuffle\n",
        "# Seed voor herhaalbaarheid\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines = []\n",
        "\n",
        "# Code van https://github.com/EagleW/Writing-editing-Network/blob/master/split_data.py\n",
        "with open(\"acl_titles_and_abstract.txt\", 'r') as file1:\n",
        "  lines=file1.readlines()\n",
        "\n",
        "abs_t = []\n",
        "abstracts = []\n",
        "titles = []\n",
        "i = 0\n",
        "# TODO: possibly generates wrong tibs/abs when encountering a blank line; remove all blank lines\n",
        "for line in lines:\n",
        "    if i % 3 == 0:\n",
        "        titles.append(line)\n",
        "    elif i % 3 == 1:\n",
        "        abstracts.append(line)\n",
        "    i += 1\n",
        "for i in range(len(abstracts)):\n",
        "    if len(titles[i]) > 0 and len(abstracts[i]) > 0:\n",
        "        h_a_pair = (titles[i], abstracts[i])\n",
        "        abs_t.append(h_a_pair)\n",
        "shuffle(abs_t)\n",
        "total = len(abs_t)\n",
        "dev = total//10\n",
        "train  = total - dev - dev\n",
        "i = 0\n",
        "\n",
        "with open(\"val.txt\", 'w') as valFile:\n",
        "  for i in range(dev):\n",
        "      valFile.writelines(abs_t[i][0])\n",
        "      valFile.writelines(abs_t[i][1])\n",
        "  #    valFile.writelines(\"\\n\")\n",
        "\n",
        "with open(\"test.txt\", 'w') as testFile:\n",
        "  for i in range(dev, 2 * dev):\n",
        "      testFile.writelines(abs_t[i][0])\n",
        "      testFile.writelines(abs_t[i][1])\n",
        "  #    file1.writelines(\"\\n\")\n",
        "\n",
        "with open(\"train.txt\", 'w') as trainFile:\n",
        "  for i in range(2 * dev, total):\n",
        "      trainFile.writelines(abs_t[i][0])\n",
        "      trainFile.writelines(abs_t[i][1])\n",
        "  #    trainFile.writelines(\"\\n\")\n"
      ],
      "metadata": {
        "id": "qnJKrpDSADgw"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_abs_and_titles_from_raw(path_to_file):\n",
        "    abstracts, titles = [], []\n",
        "    with open(path_to_file) as data:\n",
        "        lines = data.readlines()\n",
        "        for abs in lines[1::2]:\n",
        "            abstracts.append(abs.strip())\n",
        "        for title in lines[0::2]:\n",
        "            # // TODO: check if we need to add start and end tokens\n",
        "            titles.append(title.strip())\n",
        "            # titles.append('[START] ' + title.strip() + ' [END]')\n",
        "    return abstracts, titles"
      ],
      "metadata": {
        "id": "39UNm84FHK5Z"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text, train_labels = get_abs_and_titles_from_raw('train.txt')\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_text, train_labels))\n",
        "# ((train_text, train_inputs), train_labels) <-- tokenized"
      ],
      "metadata": {
        "id": "IufsyM7CHt_i"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for abs, tit in train_dataset.batch(3).take(1):\n",
        "    print('> Abstract examples:')\n",
        "    for i, a in enumerate(abs.numpy()):\n",
        "        print(i+1, a.decode('utf-8'))\n",
        "        \n",
        "    print()\n",
        "    \n",
        "    print('> Title examples:')\n",
        "    for i, t in enumerate(tit.numpy()):\n",
        "        print(i+1, t.decode('utf-8'))\n"
      ],
      "metadata": {
        "id": "mzLBGVwDJ5hC",
        "outputId": "a307e64a-6489-4f8f-cbd0-9b1d66e04ab1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Abstract examples:\n",
            "1 in this paper , we describe our participation in the tempeval-3 challenge . with our multilingual temporal tagger heideltime , we addressed task a , the extraction and normalization of temporal expressions for english and spanish . exploiting heideltimes strict separation between source code and languagedependent parts , we tuned heideltimes existing english resources and developed new spanish resources . for both languages , we achieved the best results among all participants for task a , the combination of extraction and normalization . both the improved english and the new spanish resources are publicly available with heideltime .\n",
            "2 we present three novel methods of compactly storing very large n-gram language models . these methods use substantially less space than all known approaches and allow n-gram probabilities or counts to be retrieved in constant time , at speeds comparable to modern language modeling toolkits . our basic approach generates an explicit minimal perfect hash function , that maps all n-grams in a model to distinct integers to enable storage of associated values . extensions of this approach exploit distributional characteristics of n-gram data to reduce storage costs , including variable length coding of values and the use of tiered structures that partition the data for more efficient storage . we apply our approach to storing the full google web1t n-gram set and all 1-to-5 grams of the gigaword newswire corpus . for the 1.5 billion n-grams of gigaword , for example , we can store full count information at a cost of 1.66 bytes per n-gram ( around 30 % of the cost when using the current stateof-the-art approach ) , or quantized counts for 1.41 bytes per n-gram . for applications that are tolerant of a certain class of relatively innocuous errors ( where unseen n-grams may be accepted as rare n-grams ) , we can reduce the latter cost to below 1 byte per n-gram .\n",
            "3 a poll consists of a question and a set of predefined answers from which voters can select . we present the new problem of vote prediction on comments , which involves determining which of these answers a voter selected given a comment she wrote after voting . to address this task , we exploit not only the information extracted from the comments but also extra-textual information such as user demographic information and inter-comment constraints . in an evaluation involving nearly one million comments collected from the popular sodahead social polling website , we show that a vote prediction system that exploits only textual information can be improved significantly when extended with extra-textual information .\n",
            "\n",
            "> Title examples:\n",
            "1 heideltime : tuning english and developing spanish resources\n",
            "2 storing the web in memory : space efficient language models with constant time retrieval\n",
            "3 vote prediction on comments in social polls\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.tensorflow.org/tutorials/load_data/text#example_1_predict_the_tag_for_a_stack_overflow_question\n",
        "VOCAB_SIZE = 10000\n",
        "MAX_SEQUENCE_LENGTH = 250\n",
        "\n",
        "# int_vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "#     max_tokens=VOCAB_SIZE,\n",
        "#     output_mode='int',\n",
        "#     output_sequence_length=MAX_SEQUENCE_LENGTH\n",
        "#     )\n",
        "\n",
        "int_vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "    output_mode='int'\n",
        "    )\n",
        "\n",
        "int_vectorize_layer.adapt(train_dataset)\n",
        "print(len(int_vectorize_layer.get_vocabulary()))\n"
      ],
      "metadata": {
        "id": "sv9eKCJpJ6ER",
        "outputId": "a7a930d0-1b14-4f83-c9f5-55708a8fdf19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.tensorflow.org/tutorials/load_data/text#prepare_the_dataset_for_training\n",
        "def int_vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return int_vectorize_layer(text), label\n",
        "\n",
        "def tokenize(text):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return int_vectorize_layer(text)"
      ],
      "metadata": {
        "id": "EusdLzuIK2I1"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve a batch (of 32 abstracts and titles) from the dataset.\n",
        "text_batch, label_batch = next(iter(train_dataset.batch(32)))\n",
        "first_abstract, first_label = text_batch[0], label_batch[0]\n"
      ],
      "metadata": {
        "id": "RhJV6mg-K7FY",
        "outputId": "b721f66d-74dd-4926-af73-0f8dd99c0850",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Abstract\n",
            " tf.Tensor(b'in this paper , we describe our participation in the tempeval-3 challenge . with our multilingual temporal tagger heideltime , we addressed task a , the extraction and normalization of temporal expressions for english and spanish . exploiting heideltimes strict separation between source code and languagedependent parts , we tuned heideltimes existing english resources and developed new spanish resources . for both languages , we achieved the best results among all participants for task a , the combination of extraction and normalization . both the improved english and the new spanish resources are publicly available with heideltime .', shape=(), dtype=string)\n",
            "> Title\n",
            " tf.Tensor(b'heideltime : tuning english and developing spanish resources', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "v-x8BU8oLN1D"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "\n",
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n"
      ],
      "metadata": {
        "id": "lJr8P415LVnu",
        "outputId": "cf2fadc9-d0af-4d46-f227-112e6669f12e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-b4a43b5b8abe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Length of the vocabulary in StringLookup Layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids_from_chars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# The embedding dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0membedding_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ids_from_chars' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-mxorSKBMFci"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colaboratory",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}